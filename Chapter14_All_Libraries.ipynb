{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNeII0Ldg6rAIff/4bB0qA3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ncr5012/EmailRL/blob/main/Chapter14_All_Libraries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghEGL874s4bv"
      },
      "source": [
        "#This is the code required to run the chatbot in chapter 14\n",
        "#The purpose of this is to understand how NLP works in RL to facilitate the creation of a limited GI email agent\n",
        "\n",
        "#Initialization Section - Import libraries, define utility functions\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import itertools\n",
        "import pickle\n",
        "\n",
        "import string\n",
        "from nltk.translate import bleu_score\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calc_bleu_many(cand_seq, ref_sequences):\n",
        "    sf = bleu_score.SmoothingFunction()\n",
        "    return bleu_score.sentence_bleu(ref_sequences, cand_seq,\n",
        "                                    smoothing_function=sf.method1,\n",
        "                                    weights=(0.5, 0.5))\n",
        "\n",
        "\n",
        "def calc_bleu(cand_seq, ref_seq):\n",
        "    return calc_bleu_many(cand_seq, [ref_seq])\n",
        "\n",
        "\n",
        "def tokenize(s):\n",
        "    return TweetTokenizer(preserve_case=False).tokenize(s)\n",
        "\n",
        "\n",
        "def untokenize(words):\n",
        "    to_pad = lambda t: not t.startswith(\"'\") and \\\n",
        "                       t not in string.punctuation\n",
        "    return \"\".join([\n",
        "        (\" \" + i) if to_pad(i) else i\n",
        "        for i in words\n",
        "    ]).strip()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7yBRLfdSCGC",
        "outputId": "ac341aa8-e392-4828-b761-38b049754cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "source": [
        "#load the data\n",
        "url1 = 'https://raw.githubusercontent.com/ncr5012/EmailRL/main/cornell%20movie-dialogs%20corpus/movie_characters_metadata.txt'\n",
        "movie_characters_metadata = pd.read_csv(url1,sep=\"+++$+++\")\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6abeeeefd8dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://raw.githubusercontent.com/ncr5012/EmailRL/main/cornell%20movie-dialogs%20corpus/movie_characters_metadata.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmovie_characters_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"+++$+++\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Dataset is now stored in a Pandas Dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0;34m'are \"c\", \"python\", or \"python-fwf\")'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 )\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   2405\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_original_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2406\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2407\u001b[0;31m             ) = self._infer_columns()\n\u001b[0m\u001b[1;32m   2408\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_infer_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffered_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_pos\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_buffered_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2859\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2860\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2861\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2863\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_for_bom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_next_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m                 \u001b[0morig_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_iter_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_next_iter_line\u001b[0;34m(self, row_num)\u001b[0m\n\u001b[1;32m   3014\u001b[0m         \"\"\"\n\u001b[1;32m   3015\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3016\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3017\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn_bad_lines\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_bad_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2543\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2544\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2545\u001b[0;31m                 \u001b[0mpat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2547\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/re.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;34m\"Compile a regular expression pattern, returning a Pattern object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(str, flags, pattern)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0;32m--> 420\u001b[0;31m                            not nested and not items))\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mAT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 raise source.error(\"nothing to repeat\",\n\u001b[0;32m--> 645\u001b[0;31m                                    source.tell() - here + len(this))\n\u001b[0m\u001b[1;32m    646\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_REPEATCODES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m                 raise source.error(\"multiple repeat\",\n",
            "\u001b[0;31merror\u001b[0m: nothing to repeat at position 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t74P4AnQLZlA"
      },
      "source": [
        "#cornell.py - low level data cleaner \n",
        "\"\"\"\n",
        "Cornel Movies Dialogs Corpus\n",
        "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
        "\"\"\"\n",
        "\n",
        "log = logging.getLogger(\"cornell\")\n",
        "DATA_DIR = \"data/cornell\"\n",
        "SEPARATOR = \"+++$+++\"\n",
        "\n",
        "\n",
        "def load_dialogues(data_dir=DATA_DIR, genre_filter=''):\n",
        "    \"\"\"\n",
        "    Load dialogues from cornell data\n",
        "    :return: list of list of list of words\n",
        "    \"\"\"\n",
        "    movie_set = None\n",
        "    if genre_filter:\n",
        "        movie_set = read_movie_set(data_dir, genre_filter)\n",
        "        log.info(\"Loaded %d movies with genre %s\", len(movie_set), genre_filter)\n",
        "    log.info(\"Read and tokenise phrases...\")\n",
        "    lines = read_phrases(data_dir, movies=movie_set)\n",
        "    log.info(\"Loaded %d phrases\", len(lines))\n",
        "    dialogues = load_conversations(data_dir, lines, movie_set)\n",
        "    return dialogues\n",
        "\n",
        "\n",
        "def iterate_entries(data_dir, file_name):\n",
        "    with open(os.path.join(data_dir, file_name), \"rb\") as fd:\n",
        "        for l in fd:\n",
        "            l = str(l, encoding='utf-8', errors='ignore')\n",
        "            yield list(map(str.strip, l.split(SEPARATOR)))\n",
        "\n",
        "\n",
        "def read_movie_set(data_dir, genre_filter):\n",
        "    res = set()\n",
        "    for parts in iterate_entries(data_dir, \"movie_titles_metadata.txt\"):\n",
        "        m_id, m_genres = parts[0], parts[5]\n",
        "        if m_genres.find(genre_filter) != -1:\n",
        "            res.add(m_id)\n",
        "    return res\n",
        "\n",
        "\n",
        "def read_phrases(data_dir, movies=None):\n",
        "    res = {}\n",
        "    for parts in iterate_entries(data_dir, \"movie_lines.txt\"):\n",
        "        l_id, m_id, l_str = parts[0], parts[2], parts[4]\n",
        "        if movies and m_id not in movies:\n",
        "            continue\n",
        "        tokens = utils.tokenize(l_str)\n",
        "        if tokens:\n",
        "            res[l_id] = tokens\n",
        "    return res\n",
        "\n",
        "\n",
        "def load_conversations(data_dir, lines, movies=None):\n",
        "    res = []\n",
        "    for parts in iterate_entries(data_dir, \"movie_conversations.txt\"):\n",
        "        m_id, dial_s = parts[2], parts[3]\n",
        "        if movies and m_id not in movies:\n",
        "            continue\n",
        "        l_ids = dial_s.strip(\"[]\").split(\", \")\n",
        "        l_ids = list(map(lambda s: s.strip(\"'\"), l_ids))\n",
        "        dial = [lines[l_id] for l_id in l_ids if l_id in lines]\n",
        "        if dial:\n",
        "            res.append(dial)\n",
        "    return res\n",
        "\n",
        "\n",
        "def read_genres(data_dir):\n",
        "    res = {}\n",
        "    for parts in iterate_entries(data_dir, \"movie_titles_metadata.txt\"):\n",
        "        m_id, m_genres = parts[0], parts[5]\n",
        "        l_genres = m_genres.strip(\"[]\").split(\", \")\n",
        "        l_genres = list(map(lambda s: s.strip(\"'\"), l_genres))\n",
        "        res[m_id] = l_genres\n",
        "    return res\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdD81dipKpxG"
      },
      "source": [
        "#data.py - high level data cleaner\n",
        "\n",
        "UNKNOWN_TOKEN = '#UNK'\n",
        "BEGIN_TOKEN = \"#BEG\"\n",
        "END_TOKEN = \"#END\"\n",
        "MAX_TOKENS = 20\n",
        "MIN_TOKEN_FEQ = 10\n",
        "SHUFFLE_SEED = 5871\n",
        "\n",
        "EMB_DICT_NAME = \"emb_dict.dat\"\n",
        "EMB_NAME = \"emb.npy\"\n",
        "\n",
        "log = logging.getLogger(\"data\")\n",
        "\n",
        "\n",
        "def save_emb_dict(dir_name, emb_dict):\n",
        "    with open(os.path.join(dir_name, EMB_DICT_NAME), \"wb\") as fd:\n",
        "        pickle.dump(emb_dict, fd)\n",
        "\n",
        "\n",
        "def load_emb_dict(dir_name):\n",
        "    with open(os.path.join(dir_name, EMB_DICT_NAME), \"rb\") as fd:\n",
        "        return pickle.load(fd)\n",
        "\n",
        "\n",
        "def encode_words(words, emb_dict):\n",
        "    \"\"\"\n",
        "    Convert list of words into list of embeddings indices, adding our tokens\n",
        "    :param words: list of strings\n",
        "    :param emb_dict: embeddings dictionary\n",
        "    :return: list of IDs\n",
        "    \"\"\"\n",
        "    res = [emb_dict[BEGIN_TOKEN]]\n",
        "    unk_idx = emb_dict[UNKNOWN_TOKEN]\n",
        "    for w in words:\n",
        "        idx = emb_dict.get(w.lower(), unk_idx)\n",
        "        res.append(idx)\n",
        "    res.append(emb_dict[END_TOKEN])\n",
        "    return res\n",
        "\n",
        "\n",
        "def encode_phrase_pairs(phrase_pairs, emb_dict, filter_unknows=True):\n",
        "    \"\"\"\n",
        "    Convert list of phrase pairs to training data\n",
        "    :param phrase_pairs: list of (phrase, phrase)\n",
        "    :param emb_dict: embeddings dictionary (word -> id)\n",
        "    :return: list of tuples ([input_id_seq], [output_id_seq])\n",
        "    \"\"\"\n",
        "    unk_token = emb_dict[UNKNOWN_TOKEN]\n",
        "    result = []\n",
        "    for p1, p2 in phrase_pairs:\n",
        "        p = encode_words(p1, emb_dict), encode_words(p2, emb_dict)\n",
        "        if unk_token in p[0] or unk_token in p[1]:\n",
        "            continue\n",
        "        result.append(p)\n",
        "    return result\n",
        "\n",
        "\n",
        "def group_train_data(training_data):\n",
        "    \"\"\"\n",
        "    Group training pairs by first phrase\n",
        "    :param training_data: list of (seq1, seq2) pairs\n",
        "    :return: list of (seq1, [seq*]) pairs\n",
        "    \"\"\"\n",
        "    groups = collections.defaultdict(list)\n",
        "    for p1, p2 in training_data:\n",
        "        l = groups[tuple(p1)]\n",
        "        l.append(p2)\n",
        "    return list(groups.items())\n",
        "\n",
        "\n",
        "def iterate_batches(data, batch_size):\n",
        "    assert isinstance(data, list)\n",
        "    assert isinstance(batch_size, int)\n",
        "\n",
        "    ofs = 0\n",
        "    while True:\n",
        "        batch = data[ofs*batch_size:(ofs+1)*batch_size]\n",
        "        if len(batch) <= 1:\n",
        "            break\n",
        "        yield batch\n",
        "        ofs += 1\n",
        "\n",
        "\n",
        "def load_data(genre_filter, max_tokens=MAX_TOKENS, min_token_freq=MIN_TOKEN_FEQ):\n",
        "    dialogues = cornell.load_dialogues(genre_filter=genre_filter)\n",
        "    if not dialogues:\n",
        "        log.error(\"No dialogues found, exit!\")\n",
        "        sys.exit()\n",
        "    log.info(\"Loaded %d dialogues with %d phrases, generating training pairs\",\n",
        "             len(dialogues), sum(map(len, dialogues)))\n",
        "    phrase_pairs = dialogues_to_pairs(dialogues, max_tokens=max_tokens)\n",
        "    log.info(\"Counting freq of words...\")\n",
        "    word_counts = collections.Counter()\n",
        "    for dial in dialogues:\n",
        "        for p in dial:\n",
        "            word_counts.update(p)\n",
        "    freq_set = set(map(lambda p: p[0], filter(lambda p: p[1] >= min_token_freq, word_counts.items())))\n",
        "    log.info(\"Data has %d uniq words, %d of them occur more than %d\",\n",
        "             len(word_counts), len(freq_set), min_token_freq)\n",
        "    phrase_dict = phrase_pairs_dict(phrase_pairs, freq_set)\n",
        "    return phrase_pairs, phrase_dict\n",
        "\n",
        "\n",
        "def phrase_pairs_dict(phrase_pairs, freq_set):\n",
        "    \"\"\"\n",
        "    Return the dict of words in the dialogues mapped to their IDs\n",
        "    :param phrase_pairs: list of (phrase, phrase) pairs\n",
        "    :return: dict\n",
        "    \"\"\"\n",
        "    res = {UNKNOWN_TOKEN: 0, BEGIN_TOKEN: 1, END_TOKEN: 2}\n",
        "    next_id = 3\n",
        "    for p1, p2 in phrase_pairs:\n",
        "        for w in map(str.lower, itertools.chain(p1, p2)):\n",
        "            if w not in res and w in freq_set:\n",
        "                res[w] = next_id\n",
        "                next_id += 1\n",
        "    return res\n",
        "\n",
        "\n",
        "def dialogues_to_pairs(dialogues, max_tokens=None):\n",
        "    \"\"\"\n",
        "    Convert dialogues to training pairs of phrases\n",
        "    :param dialogues:\n",
        "    :param max_tokens: limit of tokens in both question and reply\n",
        "    :return: list of (phrase, phrase) pairs\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for dial in dialogues:\n",
        "        prev_phrase = None\n",
        "        for phrase in dial:\n",
        "            if prev_phrase is not None:\n",
        "                if max_tokens is None or (len(prev_phrase) <= max_tokens and len(phrase) <= max_tokens):\n",
        "                    result.append((prev_phrase, phrase))\n",
        "            prev_phrase = phrase\n",
        "    return result\n",
        "\n",
        "\n",
        "def decode_words(indices, rev_emb_dict):\n",
        "    return [rev_emb_dict.get(idx, UNKNOWN_TOKEN) for idx in indices]\n",
        "\n",
        "\n",
        "def trim_tokens_seq(tokens, end_token):\n",
        "    res = []\n",
        "    for t in tokens:\n",
        "        res.append(t)\n",
        "        if t == end_token:\n",
        "            break\n",
        "    return res\n",
        "\n",
        "\n",
        "def split_train_test(data, train_ratio=0.95):\n",
        "    count = int(len(data) * train_ratio)\n",
        "    return data[:count], data[count:]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O68QSozFNIX2"
      },
      "source": [
        "#model.py - used to...\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from . import utils\n",
        "\n",
        "HIDDEN_STATE_SIZE = 512\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "\n",
        "class PhraseModel(nn.Module):\n",
        "    def __init__(self, emb_size, dict_size, hid_size):\n",
        "        super(PhraseModel, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(\n",
        "            num_embeddings=dict_size, embedding_dim=emb_size)\n",
        "        self.encoder = nn.LSTM(\n",
        "            input_size=emb_size, hidden_size=hid_size,\n",
        "            num_layers=1, batch_first=True)\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=emb_size, hidden_size=hid_size,\n",
        "            num_layers=1, batch_first=True)\n",
        "        self.output = nn.Linear(hid_size, dict_size)\n",
        "\n",
        "    def encode(self, x):\n",
        "        _, hid = self.encoder(x)\n",
        "        return hid\n",
        "\n",
        "    def get_encoded_item(self, encoded, index):\n",
        "        # For RNN\n",
        "        # return encoded[:, index:index+1]\n",
        "        # For LSTM\n",
        "        return encoded[0][:, index:index+1].contiguous(), \\\n",
        "               encoded[1][:, index:index+1].contiguous()\n",
        "\n",
        "    def decode_teacher(self, hid, input_seq):\n",
        "        # Method assumes batch of size=1\n",
        "        out, _ = self.decoder(input_seq, hid)\n",
        "        out = self.output(out.data)\n",
        "        return out\n",
        "\n",
        "    def decode_one(self, hid, input_x):\n",
        "        out, new_hid = self.decoder(input_x.unsqueeze(0), hid)\n",
        "        out = self.output(out)\n",
        "        return out.squeeze(dim=0), new_hid\n",
        "\n",
        "    def decode_chain_argmax(self, hid, begin_emb, seq_len,\n",
        "                            stop_at_token=None):\n",
        "        \"\"\"\n",
        "        Decode sequence by feeding predicted token to the net again. Act greedily\n",
        "        \"\"\"\n",
        "        res_logits = []\n",
        "        res_tokens = []\n",
        "        cur_emb = begin_emb\n",
        "\n",
        "        for _ in range(seq_len):\n",
        "            out_logits, hid = self.decode_one(hid, cur_emb)\n",
        "            out_token_v = torch.max(out_logits, dim=1)[1]\n",
        "            out_token = out_token_v.data.cpu().numpy()[0]\n",
        "\n",
        "            cur_emb = self.emb(out_token_v)\n",
        "\n",
        "            res_logits.append(out_logits)\n",
        "            res_tokens.append(out_token)\n",
        "            if stop_at_token is not None:\n",
        "                if out_token == stop_at_token:\n",
        "                    break\n",
        "        return torch.cat(res_logits), res_tokens\n",
        "\n",
        "    def decode_chain_sampling(self, hid, begin_emb, seq_len,\n",
        "                              stop_at_token=None):\n",
        "        \"\"\"\n",
        "        Decode sequence by feeding predicted token to the net again.\n",
        "        Act according to probabilities\n",
        "        \"\"\"\n",
        "        res_logits = []\n",
        "        res_actions = []\n",
        "        cur_emb = begin_emb\n",
        "\n",
        "        for _ in range(seq_len):\n",
        "            out_logits, hid = self.decode_one(hid, cur_emb)\n",
        "            out_probs_v = F.softmax(out_logits, dim=1)\n",
        "            out_probs = out_probs_v.data.cpu().numpy()[0]\n",
        "            action = int(np.random.choice(\n",
        "                out_probs.shape[0], p=out_probs))\n",
        "            action_v = torch.LongTensor([action])\n",
        "            action_v = action_v.to(begin_emb.device)\n",
        "            cur_emb = self.emb(action_v)\n",
        "\n",
        "            res_logits.append(out_logits)\n",
        "            res_actions.append(action)\n",
        "            if stop_at_token is not None:\n",
        "                if action == stop_at_token:\n",
        "                    break\n",
        "        return torch.cat(res_logits), res_actions\n",
        "\n",
        "\n",
        "def pack_batch_no_out(batch, embeddings, device=\"cpu\"):\n",
        "    assert isinstance(batch, list)\n",
        "    # Sort descending (CuDNN requirements)\n",
        "    batch.sort(key=lambda s: len(s[0]), reverse=True)\n",
        "    input_idx, output_idx = zip(*batch)\n",
        "    # create padded matrix of inputs\n",
        "    lens = list(map(len, input_idx))\n",
        "    input_mat = np.zeros((len(batch), lens[0]), dtype=np.int64)\n",
        "    for idx, x in enumerate(input_idx):\n",
        "        input_mat[idx, :len(x)] = x\n",
        "    input_v = torch.tensor(input_mat).to(device)\n",
        "    input_seq = rnn_utils.pack_padded_sequence(\n",
        "        input_v, lens, batch_first=True)\n",
        "    # lookup embeddings\n",
        "    r = embeddings(input_seq.data)\n",
        "    emb_input_seq = rnn_utils.PackedSequence(\n",
        "        r, input_seq.batch_sizes)\n",
        "    return emb_input_seq, input_idx, output_idx\n",
        "\n",
        "\n",
        "def pack_input(input_data, embeddings, device=\"cpu\"):\n",
        "    input_v = torch.LongTensor([input_data]).to(device)\n",
        "    r = embeddings(input_v)\n",
        "    return rnn_utils.pack_padded_sequence(\n",
        "        r, [len(input_data)], batch_first=True)\n",
        "\n",
        "\n",
        "def pack_batch(batch, embeddings, device=\"cpu\"):\n",
        "    emb_input_seq, input_idx, output_idx = pack_batch_no_out(\n",
        "        batch, embeddings, device)\n",
        "\n",
        "    # prepare output sequences, with end token stripped\n",
        "    output_seq_list = []\n",
        "    for out in output_idx:\n",
        "        s = pack_input(out[:-1], embeddings, device)\n",
        "        output_seq_list.append(s)\n",
        "    return emb_input_seq, output_seq_list, input_idx, output_idx\n",
        "\n",
        "\n",
        "def seq_bleu(model_out, ref_seq):\n",
        "    model_seq = torch.max(model_out.data, dim=1)[1]\n",
        "    model_seq = model_seq.cpu().numpy()\n",
        "    return utils.calc_bleu(model_seq, ref_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnb3icaJKECj"
      },
      "source": [
        "#cor_reader\n",
        "import argparse\n",
        "import collections\n",
        "\n",
        "from libbots import cornell, data\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-g\", \"--genre\", default='', help=\"Genre to show dialogs from\")\n",
        "    parser.add_argument(\"--show-genres\", action='store_true', default=False, help=\"Display genres stats\")\n",
        "    parser.add_argument(\"--show-dials\", action='store_true', default=False, help=\"Display dialogs\")\n",
        "    parser.add_argument(\"--show-train\", action='store_true', default=False, help=\"Display training pairs\")\n",
        "    parser.add_argument(\"--show-dict-freq\", action='store_true', default=False, help=\"Display dictionary frequency\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.show_genres:\n",
        "        genre_counts = collections.Counter()\n",
        "        genres = cornell.read_genres(cornell.DATA_DIR)\n",
        "        for movie, g_list in genres.items():\n",
        "            for g in g_list:\n",
        "                genre_counts[g] += 1\n",
        "        print(\"Genres:\")\n",
        "        for g, count in genre_counts.most_common():\n",
        "            print(\"%s: %d\" % (g, count))\n",
        "\n",
        "    if args.show_dials:\n",
        "        dials = cornell.load_dialogues(genre_filter=args.genre)\n",
        "        for d_idx, dial in enumerate(dials):\n",
        "            print(\"Dialog %d with %d phrases:\" % (d_idx, len(dial)))\n",
        "            for p in dial:\n",
        "                print(\" \".join(p))\n",
        "            print()\n",
        "\n",
        "    if args.show_train or args.show_dict_freq:\n",
        "        phrase_pairs, emb_dict = data.load_data(genre_filter=args.genre)\n",
        "\n",
        "    if args.show_train:\n",
        "        rev_emb_dict = {idx: word for word, idx in emb_dict.items()}\n",
        "        train_data = data.encode_phrase_pairs(phrase_pairs, emb_dict)\n",
        "        train_data = data.group_train_data(train_data)\n",
        "        unk_token = emb_dict[data.UNKNOWN_TOKEN]\n",
        "\n",
        "        print(\"Training pairs (%d total)\" % len(train_data))\n",
        "        train_data.sort(key=lambda p: len(p[1]), reverse=True)\n",
        "        for idx, (p1, p2_group) in enumerate(train_data):\n",
        "            w1 = data.decode_words(p1, rev_emb_dict)\n",
        "            w2_group = [data.decode_words(p2, rev_emb_dict) for p2 in p2_group]\n",
        "            print(\"%d:\" % idx, \" \".join(w1))\n",
        "            for w2 in w2_group:\n",
        "                print(\"%s:\" % (\" \" * len(str(idx))), \" \".join(w2))\n",
        "\n",
        "    if args.show_dict_freq:\n",
        "        words_stat = collections.Counter()\n",
        "        for p1, p2 in phrase_pairs:\n",
        "            words_stat.update(p1)\n",
        "        print(\"Frequency stats for %d tokens in the dict\" % len(emb_dict))\n",
        "        for token, count in words_stat.most_common():\n",
        "            print(\"%s: %d\" % (token, count))\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrnSi9AMtC_T"
      },
      "source": [
        "#Telegram_Bot\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import configparser\n",
        "import argparse\n",
        "\n",
        "try:\n",
        "    import telegram.ext\n",
        "except ImportError:\n",
        "    print(\"You need python-telegram-bot package installed \"\n",
        "          \"to start the bot\")\n",
        "    sys.exit()\n",
        "\n",
        "from libbots import data, model, utils\n",
        "\n",
        "import torch\n",
        "\n",
        "# Configuration file with the following contents\n",
        "# [telegram]\n",
        "# api=API_KEY\n",
        "CONFIG_DEFAULT = \"~/.config/rl_ch12_bot.ini\"\n",
        "\n",
        "log = logging.getLogger(\"telegram\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fmt = \"%(asctime)-15s %(levelname)s %(message)s\"\n",
        "    logging.basicConfig(format=fmt, level=logging.INFO)\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--config\", default=CONFIG_DEFAULT,\n",
        "        help=\"Configuration file for the bot, default=\" +\n",
        "             CONFIG_DEFAULT)\n",
        "    parser.add_argument(\n",
        "        \"-m\", \"--model\", required=True, help=\"Model to load\")\n",
        "    parser.add_argument(\n",
        "        \"--sample\", default=False, action='store_true',\n",
        "        help=\"Enable sampling mode\")\n",
        "    prog_args = parser.parse_args()\n",
        "\n",
        "    conf = configparser.ConfigParser()\n",
        "    if not conf.read(os.path.expanduser(prog_args.config)):\n",
        "        log.error(\"Configuration file %s not found\",\n",
        "                  prog_args.config)\n",
        "        sys.exit()\n",
        "\n",
        "    emb_dict = data.load_emb_dict(\n",
        "        os.path.dirname(prog_args.model))\n",
        "    log.info(\"Loaded embedded dict with %d entries\",\n",
        "             len(emb_dict))\n",
        "    rev_emb_dict = {\n",
        "        idx: word for word, idx in emb_dict.items()\n",
        "    }\n",
        "    end_token = emb_dict[data.END_TOKEN]\n",
        "\n",
        "    net = model.PhraseModel(\n",
        "        emb_size=model.EMBEDDING_DIM, dict_size=len(emb_dict),\n",
        "        hid_size=model.HIDDEN_STATE_SIZE)\n",
        "    net.load_state_dict(torch.load(prog_args.model))\n",
        "\n",
        "    def bot_func(bot, update, args):\n",
        "        text = \" \".join(args)\n",
        "        words = utils.tokenize(text)\n",
        "        seq_1 = data.encode_words(words, emb_dict)\n",
        "        input_seq = model.pack_input(seq_1, net.emb)\n",
        "        enc = net.encode(input_seq)\n",
        "        if prog_args.sample:\n",
        "            _, tokens = net.decode_chain_sampling(\n",
        "                enc, input_seq.data[0:1], seq_len=data.MAX_TOKENS,\n",
        "                stop_at_token=end_token)\n",
        "        else:\n",
        "            _, tokens = net.decode_chain_argmax(\n",
        "                enc, input_seq.data[0:1], seq_len=data.MAX_TOKENS,\n",
        "                stop_at_token=end_token)\n",
        "        if tokens[-1] == end_token:\n",
        "            tokens = tokens[:-1]\n",
        "        reply = data.decode_words(tokens, rev_emb_dict)\n",
        "        if reply:\n",
        "            reply_text = utils.untokenize(reply)\n",
        "            bot.send_message(chat_id=update.message.chat_id,\n",
        "                             text=reply_text)\n",
        "\n",
        "    updater = telegram.ext.Updater(conf['telegram']['api'])\n",
        "    updater.dispatcher.add_handler(\n",
        "        telegram.ext.CommandHandler('bot', bot_func,\n",
        "                                    pass_args=True))\n",
        "\n",
        "    log.info(\"Bot initialized, started serving\")\n",
        "    updater.start_polling()\n",
        "    updater.idle()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjhTe_kctM_m"
      },
      "source": [
        "#use_model\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "from libbots import data, model, utils\n",
        "\n",
        "import torch\n",
        "\n",
        "log = logging.getLogger(\"use\")\n",
        "\n",
        "\n",
        "def words_to_words(words, emb_dict, rev_emb_dict, net, use_sampling=False):\n",
        "    tokens = data.encode_words(words, emb_dict)\n",
        "    input_seq = model.pack_input(tokens, net.emb)\n",
        "    enc = net.encode(input_seq)\n",
        "    end_token = emb_dict[data.END_TOKEN]\n",
        "    if use_sampling:\n",
        "        _, out_tokens = net.decode_chain_sampling(enc, input_seq.data[0:1], seq_len=data.MAX_TOKENS,\n",
        "                                                  stop_at_token=end_token)\n",
        "    else:\n",
        "        _, out_tokens = net.decode_chain_argmax(enc, input_seq.data[0:1], seq_len=data.MAX_TOKENS,\n",
        "                                                stop_at_token=end_token)\n",
        "    if out_tokens[-1] == end_token:\n",
        "        out_tokens = out_tokens[:-1]\n",
        "    out_words = data.decode_words(out_tokens, rev_emb_dict)\n",
        "    return out_words\n",
        "\n",
        "\n",
        "def process_string(s, emb_dict, rev_emb_dict, net, use_sampling=False):\n",
        "    out_words = words_to_words(words, emb_dict, rev_emb_dict, net, use_sampling=use_sampling)\n",
        "    print(\" \".join(out_words))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(format=\"%(asctime)-15s %(levelname)s %(message)s\", level=logging.INFO)\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-m\", \"--model\", required=True, help=\"Model name to load\")\n",
        "    parser.add_argument(\"-s\", \"--string\", help=\"String to process, otherwise will loop\")\n",
        "    parser.add_argument(\"--sample\", default=False, action=\"store_true\", help=\"Enable sampling generation instead of argmax\")\n",
        "    parser.add_argument(\"--self\", type=int, default=1, help=\"Enable self-loop mode with given amount of phrases.\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    emb_dict = data.load_emb_dict(os.path.dirname(args.model))\n",
        "    net = model.PhraseModel(emb_size=model.EMBEDDING_DIM, dict_size=len(emb_dict), hid_size=model.HIDDEN_STATE_SIZE)\n",
        "    net.load_state_dict(torch.load(args.model))\n",
        "\n",
        "    rev_emb_dict = {idx: word for word, idx in emb_dict.items()}\n",
        "\n",
        "    while True:\n",
        "        if args.string:\n",
        "            input_string = args.string\n",
        "        else:\n",
        "            input_string = input(\">>> \")\n",
        "        if not input_string:\n",
        "            break\n",
        "\n",
        "        words = utils.tokenize(input_string)\n",
        "        for _ in range(args.self):\n",
        "            words = words_to_words(words, emb_dict, rev_emb_dict, net, use_sampling=args.sample)\n",
        "            print(utils.untokenize(words))\n",
        "\n",
        "        if args.string:\n",
        "            break\n",
        "    pass\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkTaUNGXtTQq"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "from libbots import data, model, utils\n",
        "\n",
        "import torch\n",
        "\n",
        "log = logging.getLogger(\"data_test\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(format=\"%(asctime)-15s %(levelname)s %(message)s\", level=logging.INFO)\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", required=True,\n",
        "                        help=\"Category to filter, empty string will use the full dataset\")\n",
        "    parser.add_argument(\"-m\", \"--model\", required=True, help=\"Model name to load\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    phrase_pairs, emb_dict = data.load_data(args.data)\n",
        "    log.info(\"Obtained %d phrase pairs with %d uniq words\", len(phrase_pairs), len(emb_dict))\n",
        "    train_data = data.encode_phrase_pairs(phrase_pairs, emb_dict)\n",
        "    train_data = data.group_train_data(train_data)\n",
        "    rev_emb_dict = {idx: word for word, idx in emb_dict.items()}\n",
        "\n",
        "    net = model.PhraseModel(emb_size=model.EMBEDDING_DIM, dict_size=len(emb_dict), hid_size=model.HIDDEN_STATE_SIZE)\n",
        "    net.load_state_dict(torch.load(args.model))\n",
        "\n",
        "    end_token = emb_dict[data.END_TOKEN]\n",
        "\n",
        "    seq_count = 0\n",
        "    sum_bleu = 0.0\n",
        "\n",
        "    for seq_1, targets in train_data:\n",
        "        input_seq = model.pack_input(seq_1, net.emb)\n",
        "        enc = net.encode(input_seq)\n",
        "        _, tokens = net.decode_chain_argmax(enc, input_seq.data[0:1],\n",
        "                                            seq_len=data.MAX_TOKENS, stop_at_token=end_token)\n",
        "        references = [seq[1:] for seq in targets]\n",
        "        bleu = utils.calc_bleu_many(tokens, references)\n",
        "        sum_bleu += bleu\n",
        "        seq_count += 1\n",
        "\n",
        "    log.info(\"Processed %d phrases, mean BLEU = %.4f\", seq_count, sum_bleu / seq_count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1LAeSzutg2f"
      },
      "source": [
        "#data_test\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "from libbots import data, model, utils\n",
        "\n",
        "import torch\n",
        "\n",
        "log = logging.getLogger(\"data_test\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(format=\"%(asctime)-15s %(levelname)s %(message)s\", level=logging.INFO)\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", required=True,\n",
        "                        help=\"Category to filter, empty string will use the full dataset\")\n",
        "    parser.add_argument(\"-m\", \"--model\", required=True, help=\"Model name to load\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    phrase_pairs, emb_dict = data.load_data(args.data)\n",
        "    log.info(\"Obtained %d phrase pairs with %d uniq words\", len(phrase_pairs), len(emb_dict))\n",
        "    train_data = data.encode_phrase_pairs(phrase_pairs, emb_dict)\n",
        "    train_data = data.group_train_data(train_data)\n",
        "    rev_emb_dict = {idx: word for word, idx in emb_dict.items()}\n",
        "\n",
        "    net = model.PhraseModel(emb_size=model.EMBEDDING_DIM, dict_size=len(emb_dict), hid_size=model.HIDDEN_STATE_SIZE)\n",
        "    net.load_state_dict(torch.load(args.model))\n",
        "\n",
        "    end_token = emb_dict[data.END_TOKEN]\n",
        "\n",
        "    seq_count = 0\n",
        "    sum_bleu = 0.0\n",
        "\n",
        "    for seq_1, targets in train_data:\n",
        "        input_seq = model.pack_input(seq_1, net.emb)\n",
        "        enc = net.encode(input_seq)\n",
        "        _, tokens = net.decode_chain_argmax(enc, input_seq.data[0:1],\n",
        "                                            seq_len=data.MAX_TOKENS, stop_at_token=end_token)\n",
        "        references = [seq[1:] for seq in targets]\n",
        "        bleu = utils.calc_bleu_many(tokens, references)\n",
        "        sum_bleu += bleu\n",
        "        seq_count += 1\n",
        "\n",
        "    log.info(\"Processed %d phrases, mean BLEU = %.4f\", seq_count, sum_bleu / seq_count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZdvEerptk7s"
      },
      "source": [
        "#train_crossent\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import logging\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from libbots import data, model, utils\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SAVES_DIR = \"saves\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "MAX_EPOCHES = 100\n",
        "\n",
        "log = logging.getLogger(\"train\")\n",
        "\n",
        "TEACHER_PROB = 0.5\n",
        "\n",
        "\n",
        "def run_test(test_data, net, end_token, device=\"cpu\"):\n",
        "    bleu_sum = 0.0\n",
        "    bleu_count = 0\n",
        "    for p1, p2 in test_data:\n",
        "        input_seq = model.pack_input(p1, net.emb, device)\n",
        "        enc = net.encode(input_seq)\n",
        "        _, tokens = net.decode_chain_argmax(\n",
        "            enc, input_seq.data[0:1], seq_len=data.MAX_TOKENS,\n",
        "            stop_at_token=end_token)\n",
        "        bleu_sum += utils.calc_bleu(tokens, p2[1:])\n",
        "        bleu_count += 1\n",
        "    return bleu_sum / bleu_count\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fmt = \"%(asctime)-15s %(levelname)s %(message)s\"\n",
        "    logging.basicConfig(format=fmt, level=logging.INFO)\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--data\", required=True,\n",
        "        help=\"Category to use for training. Empty \"\n",
        "             \"string to train on full dataset\")\n",
        "    parser.add_argument(\n",
        "        \"--cuda\", action='store_true', default=False,\n",
        "        help=\"Enable cuda\")\n",
        "    parser.add_argument(\n",
        "        \"-n\", \"--name\", required=True, help=\"Name of the run\")\n",
        "    args = parser.parse_args()\n",
        "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "    saves_path = os.path.join(SAVES_DIR, args.name)\n",
        "    os.makedirs(saves_path, exist_ok=True)\n",
        "\n",
        "    phrase_pairs, emb_dict = data.load_data(\n",
        "        genre_filter=args.data)\n",
        "    log.info(\"Obtained %d phrase pairs with %d uniq words\",\n",
        "             len(phrase_pairs), len(emb_dict))\n",
        "    data.save_emb_dict(saves_path, emb_dict)\n",
        "    end_token = emb_dict[data.END_TOKEN]\n",
        "    train_data = data.encode_phrase_pairs(phrase_pairs, emb_dict)\n",
        "    rand = np.random.RandomState(data.SHUFFLE_SEED)\n",
        "    rand.shuffle(train_data)\n",
        "    log.info(\"Training data converted, got %d samples\",\n",
        "             len(train_data))\n",
        "    train_data, test_data = data.split_train_test(train_data)\n",
        "    log.info(\"Train set has %d phrases, test %d\",\n",
        "             len(train_data), len(test_data))\n",
        "\n",
        "    net = model.PhraseModel(\n",
        "        emb_size=model.EMBEDDING_DIM, dict_size=len(emb_dict),\n",
        "        hid_size=model.HIDDEN_STATE_SIZE).to(device)\n",
        "    log.info(\"Model: %s\", net)\n",
        "\n",
        "    writer = SummaryWriter(comment=\"-\" + args.name)\n",
        "\n",
        "    optimiser = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "    best_bleu = None\n",
        "    for epoch in range(MAX_EPOCHES):\n",
        "        losses = []\n",
        "        bleu_sum = 0.0\n",
        "        bleu_count = 0\n",
        "        for batch in data.iterate_batches(train_data, BATCH_SIZE):\n",
        "            optimiser.zero_grad()\n",
        "            input_seq, out_seq_list, _, out_idx = \\\n",
        "                model.pack_batch(batch, net.emb, device)\n",
        "            enc = net.encode(input_seq)\n",
        "\n",
        "            net_results = []\n",
        "            net_targets = []\n",
        "            for idx, out_seq in enumerate(out_seq_list):\n",
        "                ref_indices = out_idx[idx][1:]\n",
        "                enc_item = net.get_encoded_item(enc, idx)\n",
        "                if random.random() < TEACHER_PROB:\n",
        "                    r = net.decode_teacher(enc_item, out_seq)\n",
        "                    bleu_sum += model.seq_bleu(r, ref_indices)\n",
        "                else:\n",
        "                    r, seq = net.decode_chain_argmax(\n",
        "                        enc_item, out_seq.data[0:1],\n",
        "                        len(ref_indices))\n",
        "                    bleu_sum += utils.calc_bleu(seq, ref_indices)\n",
        "                net_results.append(r)\n",
        "                net_targets.extend(ref_indices)\n",
        "                bleu_count += 1\n",
        "            results_v = torch.cat(net_results)\n",
        "            targets_v = torch.LongTensor(net_targets).to(device)\n",
        "            loss_v = F.cross_entropy(results_v, targets_v)\n",
        "            loss_v.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            losses.append(loss_v.item())\n",
        "        bleu = bleu_sum / bleu_count\n",
        "        bleu_test = run_test(test_data, net, end_token, device)\n",
        "        log.info(\"Epoch %d: mean loss %.3f, mean BLEU %.3f, \"\n",
        "                 \"test BLEU %.3f\", epoch, np.mean(losses),\n",
        "                 bleu, bleu_test)\n",
        "        writer.add_scalar(\"loss\", np.mean(losses), epoch)\n",
        "        writer.add_scalar(\"bleu\", bleu, epoch)\n",
        "        writer.add_scalar(\"bleu_test\", bleu_test, epoch)\n",
        "        if best_bleu is None or best_bleu < bleu_test:\n",
        "            if best_bleu is not None:\n",
        "                out_name = os.path.join(\n",
        "                    saves_path, \"pre_bleu_%.3f_%02d.dat\" % (\n",
        "                        bleu_test, epoch))\n",
        "                torch.save(net.state_dict(), out_name)\n",
        "                log.info(\"Best BLEU updated %.3f\", bleu_test)\n",
        "            best_bleu = bleu_test\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            out_name = os.path.join(\n",
        "                saves_path, \"epoch_%03d_%.3f_%.3f.dat\" % (\n",
        "                    epoch, bleu, bleu_test))\n",
        "            torch.save(net.state_dict(), out_name)\n",
        "\n",
        "    writer.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfp0mvmLt1y7"
      },
      "source": [
        "#train_scst\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import logging\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from libbots import data, model, utils\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import ptan\n",
        "\n",
        "SAVES_DIR = \"saves\"\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 5e-4\n",
        "MAX_EPOCHES = 10000\n",
        "\n",
        "log = logging.getLogger(\"train\")\n",
        "\n",
        "\n",
        "def run_test(test_data, net, end_token, device=\"cpu\"):\n",
        "    bleu_sum = 0.0\n",
        "    bleu_count = 0\n",
        "    for p1, p2 in test_data:\n",
        "        input_seq = model.pack_input(p1, net.emb, device)\n",
        "        enc = net.encode(input_seq)\n",
        "        _, tokens = net.decode_chain_argmax(\n",
        "            enc, input_seq.data[0:1], seq_len=data.MAX_TOKENS,\n",
        "            stop_at_token=end_token)\n",
        "        ref_indices = [\n",
        "            indices[1:]\n",
        "            for indices in p2\n",
        "        ]\n",
        "        bleu_sum += utils.calc_bleu_many(tokens, ref_indices)\n",
        "        bleu_count += 1\n",
        "    return bleu_sum / bleu_count\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    fmt = \"%(asctime)-15s %(levelname)s %(message)s\"\n",
        "    logging.basicConfig(format=fmt, level=logging.INFO)\n",
        "    parser.add_argument(\n",
        "        \"--data\", required=True,\n",
        "        help=\"Category to use for training. Empty \"\n",
        "             \"string to train on full dataset\")\n",
        "    parser.add_argument(\n",
        "        \"--cuda\", action='store_true', default=False,\n",
        "        help=\"Enable cuda\")\n",
        "    parser.add_argument(\n",
        "        \"-n\", \"--name\", required=True,\n",
        "        help=\"Name of the run\")\n",
        "    parser.add_argument(\n",
        "        \"-l\", \"--load\", required=True,\n",
        "        help=\"Load model and continue in RL mode\")\n",
        "    parser.add_argument(\n",
        "        \"--samples\", type=int, default=4,\n",
        "        help=\"Count of samples in prob mode\")\n",
        "    parser.add_argument(\n",
        "        \"--disable-skip\", default=False, action='store_true',\n",
        "        help=\"Disable skipping of samples with high argmax BLEU\")\n",
        "    args = parser.parse_args()\n",
        "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "    saves_path = os.path.join(SAVES_DIR, args.name)\n",
        "    os.makedirs(saves_path, exist_ok=True)\n",
        "\n",
        "    phrase_pairs, emb_dict = \\\n",
        "        data.load_data(genre_filter=args.data)\n",
        "    log.info(\"Obtained %d phrase pairs with %d uniq words\",\n",
        "             len(phrase_pairs), len(emb_dict))\n",
        "    data.save_emb_dict(saves_path, emb_dict)\n",
        "    end_token = emb_dict[data.END_TOKEN]\n",
        "    train_data = data.encode_phrase_pairs(phrase_pairs, emb_dict)\n",
        "    rand = np.random.RandomState(data.SHUFFLE_SEED)\n",
        "    rand.shuffle(train_data)\n",
        "    train_data, test_data = data.split_train_test(train_data)\n",
        "    log.info(\"Training data converted, got %d samples\",\n",
        "             len(train_data))\n",
        "    train_data = data.group_train_data(train_data)\n",
        "    test_data = data.group_train_data(test_data)\n",
        "    log.info(\"Train set has %d phrases, test %d\",\n",
        "             len(train_data), len(test_data))\n",
        "\n",
        "    rev_emb_dict = {idx: word for word, idx in emb_dict.items()}\n",
        "\n",
        "    net = model.PhraseModel(\n",
        "        emb_size=model.EMBEDDING_DIM, dict_size=len(emb_dict),\n",
        "        hid_size=model.HIDDEN_STATE_SIZE).to(device)\n",
        "    log.info(\"Model: %s\", net)\n",
        "\n",
        "    writer = SummaryWriter(comment=\"-\" + args.name)\n",
        "    net.load_state_dict(torch.load(args.load))\n",
        "    log.info(\"Model loaded from %s, continue \"\n",
        "             \"training in RL mode...\", args.load)\n",
        "\n",
        "    # BEGIN token\n",
        "    beg_token = torch.LongTensor([emb_dict[data.BEGIN_TOKEN]])\n",
        "    beg_token = beg_token.to(device)\n",
        "\n",
        "    with ptan.common.utils.TBMeanTracker(\n",
        "            writer, 100) as tb_tracker:\n",
        "        optimiser = optim.Adam(\n",
        "            net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n",
        "        batch_idx = 0\n",
        "        best_bleu = None\n",
        "        for epoch in range(MAX_EPOCHES):\n",
        "            random.shuffle(train_data)\n",
        "            dial_shown = False\n",
        "\n",
        "            total_samples = 0\n",
        "            skipped_samples = 0\n",
        "            bleus_argmax = []\n",
        "            bleus_sample = []\n",
        "\n",
        "            for batch in data.iterate_batches(\n",
        "                    train_data, BATCH_SIZE):\n",
        "                batch_idx += 1\n",
        "                optimiser.zero_grad()\n",
        "                input_seq, input_batch, output_batch = \\\n",
        "                    model.pack_batch_no_out(batch, net.emb, device)\n",
        "                enc = net.encode(input_seq)\n",
        "\n",
        "                net_policies = []\n",
        "                net_actions = []\n",
        "                net_advantages = []\n",
        "                beg_embedding = net.emb(beg_token)\n",
        "\n",
        "                for idx, inp_idx in enumerate(input_batch):\n",
        "                    total_samples += 1\n",
        "                    ref_indices = [\n",
        "                        indices[1:]\n",
        "                        for indices in output_batch[idx]\n",
        "                    ]\n",
        "                    item_enc = net.get_encoded_item(enc, idx)\n",
        "                    r_argmax, actions = net.decode_chain_argmax(\n",
        "                        item_enc, beg_embedding, data.MAX_TOKENS,\n",
        "                        stop_at_token=end_token)\n",
        "                    argmax_bleu = utils.calc_bleu_many(\n",
        "                        actions, ref_indices)\n",
        "                    bleus_argmax.append(argmax_bleu)\n",
        "\n",
        "                    if not args.disable_skip:\n",
        "                        if argmax_bleu > 0.99:\n",
        "                            skipped_samples += 1\n",
        "                            continue\n",
        "\n",
        "                    if not dial_shown:\n",
        "                        w = data.decode_words(\n",
        "                            inp_idx, rev_emb_dict)\n",
        "                        log.info(\"Input: %s\", utils.untokenize(w))\n",
        "                        ref_words = [\n",
        "                            utils.untokenize(\n",
        "                                data.decode_words(\n",
        "                                    ref, rev_emb_dict))\n",
        "                            for ref in ref_indices\n",
        "                        ]\n",
        "                        ref = \" ~~|~~ \".join(ref_words)\n",
        "                        log.info(\"Refer: %s\", ref)\n",
        "                        w = data.decode_words(\n",
        "                            actions, rev_emb_dict)\n",
        "                        log.info(\"Argmax: %s, bleu=%.4f\",\n",
        "                                 utils.untokenize(w), argmax_bleu)\n",
        "\n",
        "                    for _ in range(args.samples):\n",
        "                        r_sample, actions = \\\n",
        "                            net.decode_chain_sampling(\n",
        "                                item_enc, beg_embedding,\n",
        "                                data.MAX_TOKENS,\n",
        "                                stop_at_token=end_token)\n",
        "                        sample_bleu = utils.calc_bleu_many(\n",
        "                            actions, ref_indices)\n",
        "\n",
        "                        if not dial_shown:\n",
        "                            w = data.decode_words(\n",
        "                                actions, rev_emb_dict)\n",
        "                            log.info(\"Sample: %s, bleu=%.4f\",\n",
        "                                     utils.untokenize(w),\n",
        "                                     sample_bleu)\n",
        "\n",
        "                        net_policies.append(r_sample)\n",
        "                        net_actions.extend(actions)\n",
        "                        adv = sample_bleu - argmax_bleu\n",
        "                        net_advantages.extend(\n",
        "                            [adv]*len(actions))\n",
        "                        bleus_sample.append(sample_bleu)\n",
        "                    dial_shown = True\n",
        "\n",
        "                if not net_policies:\n",
        "                    continue\n",
        "\n",
        "                policies_v = torch.cat(net_policies)\n",
        "                actions_t = torch.LongTensor(\n",
        "                    net_actions).to(device)\n",
        "                adv_v = torch.FloatTensor(\n",
        "                    net_advantages).to(device)\n",
        "                log_prob_v = F.log_softmax(policies_v, dim=1)\n",
        "                lp_a = log_prob_v[range(len(net_actions)),\n",
        "                                  actions_t]\n",
        "                log_prob_actions_v = adv_v * lp_a\n",
        "                loss_policy_v = -log_prob_actions_v.mean()\n",
        "\n",
        "                loss_v = loss_policy_v\n",
        "                loss_v.backward()\n",
        "                optimiser.step()\n",
        "\n",
        "                tb_tracker.track(\"advantage\", adv_v, batch_idx)\n",
        "                tb_tracker.track(\"loss_policy\", loss_policy_v,\n",
        "                                 batch_idx)\n",
        "                tb_tracker.track(\"loss_total\", loss_v, batch_idx)\n",
        "\n",
        "            bleu_test = run_test(test_data, net,\n",
        "                                 end_token, device)\n",
        "            bleu = np.mean(bleus_argmax)\n",
        "            writer.add_scalar(\"bleu_test\", bleu_test, batch_idx)\n",
        "            writer.add_scalar(\"bleu_argmax\", bleu, batch_idx)\n",
        "            writer.add_scalar(\"bleu_sample\",\n",
        "                              np.mean(bleus_sample), batch_idx)\n",
        "            writer.add_scalar(\"skipped_samples\",\n",
        "                              skipped_samples / total_samples,\n",
        "                              batch_idx)\n",
        "            writer.add_scalar(\"epoch\", batch_idx, epoch)\n",
        "            log.info(\"Epoch %d, test BLEU: %.3f\",\n",
        "                     epoch, bleu_test)\n",
        "            if best_bleu is None or best_bleu < bleu_test:\n",
        "                best_bleu = bleu_test\n",
        "                log.info(\"Best bleu updated: %.4f\", bleu_test)\n",
        "                torch.save(net.state_dict(), os.path.join(\n",
        "                    saves_path, \"bleu_%.3f_%02d.dat\" % (\n",
        "                        bleu_test, epoch)))\n",
        "            if epoch % 10 == 0:\n",
        "                torch.save(net.state_dict(), os.path.join(\n",
        "                    saves_path, \"epoch_%03d_%.3f_%.3f.dat\" % (\n",
        "                        epoch, bleu, bleu_test)))\n",
        "\n",
        "    writer.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMinucLBuRb_"
      },
      "source": [
        "from unittest import TestCase\n",
        "\n",
        "import libbots.data\n",
        "from libbots import data, subtitles\n",
        "\n",
        "\n",
        "class TestData(TestCase):\n",
        "    emb_dict = {\n",
        "        data.BEGIN_TOKEN: 0,\n",
        "        data.END_TOKEN: 1,\n",
        "        data.UNKNOWN_TOKEN: 2,\n",
        "        'a': 3,\n",
        "        'b': 4\n",
        "    }\n",
        "\n",
        "    def test_encode_words(self):\n",
        "        res = data.encode_words(['a', 'b', 'c'], self.emb_dict)\n",
        "        self.assertEqual(res, [0, 3, 4, 2, 1])\n",
        "\n",
        "    # def test_dialogues_to_train(self):\n",
        "    #     dialogues = [\n",
        "    #         [\n",
        "    #             libbots.data.Phrase(words=['a', 'b'], time_start=0, time_stop=1),\n",
        "    #             libbots.data.Phrase(words=['b', 'a'], time_start=2, time_stop=3),\n",
        "    #             libbots.data.Phrase(words=['b', 'a'], time_start=2, time_stop=3),\n",
        "    #         ],\n",
        "    #         [\n",
        "    #             libbots.data.Phrase(words=['a', 'b'], time_start=0, time_stop=1),\n",
        "    #         ]\n",
        "    #     ]\n",
        "    #\n",
        "    #     res = data.dialogues_to_train(dialogues, self.emb_dict)\n",
        "    #     self.assertEqual(res, [\n",
        "    #         ([0, 3, 4, 1], [0, 4, 3, 1]),\n",
        "    #         ([0, 4, 3, 1], [0, 4, 3, 1]),\n",
        "    #     ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkzJr9nEuUMm"
      },
      "source": [
        "#test_subtitles\n",
        "\n",
        "import datetime\n",
        "from unittest import TestCase\n",
        "\n",
        "import libbots.data\n",
        "from libbots import subtitles\n",
        "\n",
        "\n",
        "class TestPhrases(TestCase):\n",
        "    def test_split_phrase(self):\n",
        "        phrase = libbots.data.Phrase(words=[\"a\", \"b\", \"c\"], time_start=datetime.timedelta(seconds=0),\n",
        "                                     time_stop=datetime.timedelta(seconds=10))\n",
        "        res = subtitles.split_phrase(phrase)\n",
        "        self.assertIsInstance(res, list)\n",
        "        self.assertEqual(len(res), 1)\n",
        "        self.assertEqual(res[0], phrase)\n",
        "\n",
        "        phrase = libbots.data.Phrase(words=[\"a\", \"b\", \"-\", \"c\"], time_start=datetime.timedelta(seconds=0),\n",
        "                                     time_stop=datetime.timedelta(seconds=10))\n",
        "        res = subtitles.split_phrase(phrase)\n",
        "        self.assertEqual(len(res), 2)\n",
        "        self.assertEqual(res[0].words, [\"a\", \"b\"])\n",
        "        self.assertEqual(res[1].words, [\"c\"])\n",
        "        self.assertAlmostEqual(res[0].time_start.total_seconds(), 0)\n",
        "        self.assertAlmostEqual(res[0].time_stop.total_seconds(), 5)\n",
        "        self.assertAlmostEqual(res[1].time_start.total_seconds(), 5)\n",
        "        self.assertAlmostEqual(res[1].time_stop.total_seconds(), 10)\n",
        "\n",
        "        phrase = libbots.data.Phrase(words=['-', 'Wait', 'a', 'sec', '.', '-'], time_start=datetime.timedelta(0, 588, 204000),\n",
        "                                     time_stop=datetime.timedelta(0, 590, 729000))\n",
        "        res = subtitles.split_phrase(phrase)\n",
        "        self.assertEqual(res[0].words, [\"Wait\", \"a\", \"sec\", \".\"])\n",
        "\n",
        "\n",
        "class TestUtils(TestCase):\n",
        "    def test_parse_time(self):\n",
        "        self.assertEqual(subtitles.parse_time(\"00:00:33,074\"),\n",
        "                         datetime.timedelta(seconds=33, milliseconds=74))\n",
        "\n",
        "    def test_remove_braced_words(self):\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', 'b', 'c']),\n",
        "                         ['a', 'b', 'c'])\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', '[', 'b', ']', 'c']),\n",
        "                         ['a', 'c'])\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', '[', 'b', 'c']),\n",
        "                         ['a'])\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', ']', 'b', 'c']),\n",
        "                         ['a', 'b', 'c'])\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', '(', 'b', ']', 'c']),\n",
        "                         ['a', 'c'])\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', '(', 'b', 'c']),\n",
        "                         ['a'])\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', ')', 'b', 'c']),\n",
        "                         ['a', 'b', 'c'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW0m5hDUuwkl"
      },
      "source": [
        "#data_test\n",
        "\n",
        "import libbots.data\n",
        "from libbots import data, subtitles\n",
        "\n",
        "\n",
        "class TestData(TestCase):\n",
        "    emb_dict = {\n",
        "        data.BEGIN_TOKEN: 0,\n",
        "        data.END_TOKEN: 1,\n",
        "        data.UNKNOWN_TOKEN: 2,\n",
        "        'a': 3,\n",
        "        'b': 4\n",
        "    }\n",
        "\n",
        "    def test_encode_words(self):\n",
        "        res = data.encode_words(['a', 'b', 'c'], self.emb_dict)\n",
        "        self.assertEqual(res, [0, 3, 4, 2, 1])\n",
        "\n",
        "    # def test_dialogues_to_train(self):\n",
        "    #     dialogues = [\n",
        "    #         [\n",
        "    #             libbots.data.Phrase(words=['a', 'b'], time_start=0, time_stop=1),\n",
        "    #             libbots.data.Phrase(words=['b', 'a'], time_start=2, time_stop=3),\n",
        "    #             libbots.data.Phrase(words=['b', 'a'], time_start=2, time_stop=3),\n",
        "    #         ],\n",
        "    #         [\n",
        "    #             libbots.data.Phrase(words=['a', 'b'], time_start=0, time_stop=1),\n",
        "    #         ]\n",
        "    #     ]\n",
        "    #\n",
        "    #     res = data.dialogues_to_train(dialogues, self.emb_dict)\n",
        "    #     self.assertEqual(res, [\n",
        "    #         ([0, 3, 4, 1], [0, 4, 3, 1]),\n",
        "    #         ([0, 4, 3, 1], [0, 4, 3, 1]),\n",
        "    #     ])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}