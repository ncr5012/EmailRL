{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/ncr5012/EmailRL/blob/main/Chapter14_All_Libraries.ipynb",
      "authorship_tag": "ABX9TyPtT/M4TNTkA4h3ai1McSFX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ncr5012/EmailRL/blob/main/Chapter14_All_Libraries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the code required to run the chatbot in chapter 14\n",
        "#The purpose of this is to understand how NLP works in RL to facilitate the creation of a limited GI email agent\n",
        "\n",
        "#Initialization Section - Import libraries, define utility functions\n",
        "\n",
        "!pip install tensorboardX\n",
        "\n",
        "\n",
        "import collections\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import itertools\n",
        "import pickle\n",
        "import configparser\n",
        "import random\n",
        "\n",
        "import string\n",
        "from nltk.translate import bleu_score\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "def calc_bleu_many(cand_seq, ref_sequences):\n",
        "    sf = bleu_score.SmoothingFunction()\n",
        "    return bleu_score.sentence_bleu(ref_sequences, cand_seq,\n",
        "                                    smoothing_function=sf.method1,\n",
        "                                    weights=(0.5, 0.5))\n",
        "\n",
        "\n",
        "def calc_bleu(cand_seq, ref_seq):\n",
        "    return calc_bleu_many(cand_seq, [ref_seq])\n",
        "\n",
        "\n",
        "def tokenize(s):\n",
        "    return TweetTokenizer(preserve_case=False).tokenize(s)\n",
        "\n",
        "\n",
        "def untokenize(words):\n",
        "    to_pad = lambda t: not t.startswith(\"'\") and \\\n",
        "                       t not in string.punctuation\n",
        "    return \"\".join([\n",
        "        (\" \" + i) if to_pad(i) else i\n",
        "        for i in words\n",
        "    ]).strip()"
      ],
      "metadata": {
        "id": "4aSz5ZMCLtPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e1f494-bb89-4008-a24e-4713b7a87726"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t74P4AnQLZlA"
      },
      "source": [
        "#cornell.py - low level data cleaner \n",
        "\"\"\"\n",
        "Cornell Movies Dialogs Corpus\n",
        "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
        "\"\"\"\n",
        "log = logging.getLogger(\"cornell\")\n",
        "DATA_DIR = \"/content/drive/MyDrive/Cornell_Movie_Data\"\n",
        "SEPARATOR = \"+++$+++\"\n",
        "\n",
        "\n",
        "def load_dialogues(data_dir=DATA_DIR, genre_filter=''):\n",
        "    \"\"\"\n",
        "    Load dialogues from cornell data\n",
        "    :return: list of list of list of words\n",
        "    \"\"\"\n",
        "    movie_set = None\n",
        "    if genre_filter:\n",
        "        movie_set = read_movie_set(data_dir, genre_filter)\n",
        "        log.info(\"Loaded %d movies with genre %s\", len(movie_set), genre_filter)\n",
        "    log.info(\"Read and tokenise phrases...\")\n",
        "    lines = read_phrases(data_dir, movies=movie_set)\n",
        "    log.info(\"Loaded %d phrases\", len(lines))\n",
        "    dialogues = load_conversations(data_dir, lines, movie_set)\n",
        "    return dialogues\n",
        "\n",
        "\n",
        "def iterate_entries(data_dir, file_name):\n",
        "    with open(os.path.join(data_dir, file_name), \"rb\") as fd:\n",
        "        for l in fd:\n",
        "            l = str(l, encoding='utf-8', errors='ignore')\n",
        "            yield list(map(str.strip, l.split(SEPARATOR)))\n",
        "\n",
        "\n",
        "def read_movie_set(data_dir, genre_filter):\n",
        "    res = set()\n",
        "    for parts in iterate_entries(data_dir, \"movie_titles_metadata.txt\"):\n",
        "        m_id, m_genres = parts[0], parts[5]\n",
        "        if m_genres.find(genre_filter) != -1:\n",
        "            res.add(m_id)\n",
        "    return res\n",
        "\n",
        "\n",
        "def read_phrases(data_dir, movies=None):\n",
        "    res = {}\n",
        "    for parts in iterate_entries(data_dir, \"movie_lines.txt\"):\n",
        "        l_id, m_id, l_str = parts[0], parts[2], parts[4]\n",
        "        if movies and m_id not in movies:\n",
        "            continue\n",
        "        tokens = tokenize(l_str)\n",
        "        if tokens:\n",
        "            res[l_id] = tokens\n",
        "    return res\n",
        "\n",
        "\n",
        "def load_conversations(data_dir, lines, movies=None):\n",
        "    res = []\n",
        "    for parts in iterate_entries(data_dir, \"movie_conversations.txt\"):\n",
        "        m_id, dial_s = parts[2], parts[3]\n",
        "        if movies and m_id not in movies:\n",
        "            continue\n",
        "        l_ids = dial_s.strip(\"[]\").split(\", \")\n",
        "        l_ids = list(map(lambda s: s.strip(\"'\"), l_ids))\n",
        "        dial = [lines[l_id] for l_id in l_ids if l_id in lines]\n",
        "        if dial:\n",
        "            res.append(dial)\n",
        "    return res\n",
        "\n",
        "\n",
        "def read_genres(data_dir):\n",
        "    res = {}\n",
        "    for parts in iterate_entries(data_dir, \"movie_titles_metadata.txt\"):\n",
        "        m_id, m_genres = parts[0], parts[5]\n",
        "        l_genres = m_genres.strip(\"[]\").split(\", \")\n",
        "        l_genres = list(map(lambda s: s.strip(\"'\"), l_genres))\n",
        "        res[m_id] = l_genres\n",
        "    return res\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdD81dipKpxG"
      },
      "source": [
        "#data.py - high level data cleaner\n",
        "\n",
        "UNKNOWN_TOKEN = '#UNK'\n",
        "BEGIN_TOKEN = \"#BEG\"\n",
        "END_TOKEN = \"#END\"\n",
        "MAX_TOKENS = 20\n",
        "MIN_TOKEN_FEQ = 10\n",
        "SHUFFLE_SEED = 5871\n",
        "\n",
        "EMB_DICT_NAME = \"emb_dict.dat\"\n",
        "EMB_NAME = \"emb.npy\"\n",
        "\n",
        "log = logging.getLogger(\"data\")\n",
        "\n",
        "\n",
        "def save_emb_dict(dir_name, emb_dict):\n",
        "    with open(os.path.join(dir_name, EMB_DICT_NAME), \"wb\") as fd:\n",
        "        pickle.dump(emb_dict, fd)\n",
        "\n",
        "\n",
        "def load_emb_dict(dir_name):\n",
        "    with open(os.path.join(dir_name, EMB_DICT_NAME), \"rb\") as fd:\n",
        "        return pickle.load(fd)\n",
        "\n",
        "\n",
        "def encode_words(words, emb_dict):\n",
        "    \"\"\"\n",
        "    Convert list of words into list of embeddings indices, adding our tokens\n",
        "    :param words: list of strings\n",
        "    :param emb_dict: embeddings dictionary\n",
        "    :return: list of IDs\n",
        "    \"\"\"\n",
        "    res = [emb_dict[BEGIN_TOKEN]]\n",
        "    unk_idx = emb_dict[UNKNOWN_TOKEN]\n",
        "    for w in words:\n",
        "        idx = emb_dict.get(w.lower(), unk_idx)\n",
        "        res.append(idx)\n",
        "    res.append(emb_dict[END_TOKEN])\n",
        "    return res\n",
        "\n",
        "\n",
        "def encode_phrase_pairs(phrase_pairs, emb_dict, filter_unknows=True):\n",
        "    \"\"\"\n",
        "    Convert list of phrase pairs to training data\n",
        "    :param phrase_pairs: list of (phrase, phrase)\n",
        "    :param emb_dict: embeddings dictionary (word -> id)\n",
        "    :return: list of tuples ([input_id_seq], [output_id_seq])\n",
        "    \"\"\"\n",
        "    unk_token = emb_dict[UNKNOWN_TOKEN]\n",
        "    result = []\n",
        "    for p1, p2 in phrase_pairs:\n",
        "        p = encode_words(p1, emb_dict), encode_words(p2, emb_dict)\n",
        "        if unk_token in p[0] or unk_token in p[1]:\n",
        "            continue\n",
        "        result.append(p)\n",
        "    return result\n",
        "\n",
        "\n",
        "def group_train_data(training_data):\n",
        "    \"\"\"\n",
        "    Group training pairs by first phrase\n",
        "    :param training_data: list of (seq1, seq2) pairs\n",
        "    :return: list of (seq1, [seq*]) pairs\n",
        "    \"\"\"\n",
        "    groups = collections.defaultdict(list)\n",
        "    for p1, p2 in training_data:\n",
        "        l = groups[tuple(p1)]\n",
        "        l.append(p2)\n",
        "    return list(groups.items())\n",
        "\n",
        "\n",
        "def iterate_batches(data, batch_size):\n",
        "    assert isinstance(data, list)\n",
        "    assert isinstance(batch_size, int)\n",
        "\n",
        "    ofs = 0\n",
        "    while True:\n",
        "        batch = data[ofs*batch_size:(ofs+1)*batch_size]\n",
        "        if len(batch) <= 1:\n",
        "            break\n",
        "        yield batch\n",
        "        ofs += 1\n",
        "\n",
        "\n",
        "def load_data(genre_filter, max_tokens=MAX_TOKENS, min_token_freq=MIN_TOKEN_FEQ):\n",
        "    dialogues = load_dialogues(genre_filter=genre_filter)\n",
        "    if not dialogues:\n",
        "        log.error(\"No dialogues found, exit!\")\n",
        "        sys.exit()\n",
        "    log.info(\"Loaded %d dialogues with %d phrases, generating training pairs\",\n",
        "             len(dialogues), sum(map(len, dialogues)))\n",
        "    phrase_pairs = dialogues_to_pairs(dialogues, max_tokens=max_tokens)\n",
        "    log.info(\"Counting freq of words...\")\n",
        "    word_counts = collections.Counter()\n",
        "    for dial in dialogues:\n",
        "        for p in dial:\n",
        "            word_counts.update(p)\n",
        "    freq_set = set(map(lambda p: p[0], filter(lambda p: p[1] >= min_token_freq, word_counts.items())))\n",
        "    log.info(\"Data has %d uniq words, %d of them occur more than %d\",\n",
        "             len(word_counts), len(freq_set), min_token_freq)\n",
        "    phrase_dict = phrase_pairs_dict(phrase_pairs, freq_set)\n",
        "    return phrase_pairs, phrase_dict\n",
        "\n",
        "\n",
        "def phrase_pairs_dict(phrase_pairs, freq_set):\n",
        "    \"\"\"\n",
        "    Return the dict of words in the dialogues mapped to their IDs\n",
        "    :param phrase_pairs: list of (phrase, phrase) pairs\n",
        "    :return: dict\n",
        "    \"\"\"\n",
        "    res = {UNKNOWN_TOKEN: 0, BEGIN_TOKEN: 1, END_TOKEN: 2}\n",
        "    next_id = 3\n",
        "    for p1, p2 in phrase_pairs:\n",
        "        for w in map(str.lower, itertools.chain(p1, p2)):\n",
        "            if w not in res and w in freq_set:\n",
        "                res[w] = next_id\n",
        "                next_id += 1\n",
        "    return res\n",
        "\n",
        "\n",
        "def dialogues_to_pairs(dialogues, max_tokens=None):\n",
        "    \"\"\"\n",
        "    Convert dialogues to training pairs of phrases\n",
        "    :param dialogues:\n",
        "    :param max_tokens: limit of tokens in both question and reply\n",
        "    :return: list of (phrase, phrase) pairs\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for dial in dialogues:\n",
        "        prev_phrase = None\n",
        "        for phrase in dial:\n",
        "            if prev_phrase is not None:\n",
        "                if max_tokens is None or (len(prev_phrase) <= max_tokens and len(phrase) <= max_tokens):\n",
        "                    result.append((prev_phrase, phrase))\n",
        "            prev_phrase = phrase\n",
        "    return result\n",
        "\n",
        "\n",
        "def decode_words(indices, rev_emb_dict):\n",
        "    return [rev_emb_dict.get(idx, UNKNOWN_TOKEN) for idx in indices]\n",
        "\n",
        "\n",
        "def trim_tokens_seq(tokens, end_token):\n",
        "    res = []\n",
        "    for t in tokens:\n",
        "        res.append(t)\n",
        "        if t == end_token:\n",
        "            break\n",
        "    return res\n",
        "\n",
        "\n",
        "def split_train_test(data, train_ratio=0.95):\n",
        "    count = int(len(data) * train_ratio)\n",
        "    return data[:count], data[count:]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O68QSozFNIX2"
      },
      "source": [
        "#model.py - used to...\n",
        "\n",
        "\n",
        "HIDDEN_STATE_SIZE = 512\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "\n",
        "class PhraseModel(nn.Module):\n",
        "    def __init__(self, emb_size, dict_size, hid_size):\n",
        "        super(PhraseModel, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(\n",
        "            num_embeddings=dict_size, embedding_dim=emb_size)\n",
        "        self.encoder = nn.LSTM(\n",
        "            input_size=emb_size, hidden_size=hid_size,\n",
        "            num_layers=1, batch_first=True)\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=emb_size, hidden_size=hid_size,\n",
        "            num_layers=1, batch_first=True)\n",
        "        self.output = nn.Linear(hid_size, dict_size)\n",
        "\n",
        "    def encode(self, x):\n",
        "        _, hid = self.encoder(x)\n",
        "        return hid\n",
        "\n",
        "    def get_encoded_item(self, encoded, index):\n",
        "        # For RNN\n",
        "        # return encoded[:, index:index+1]\n",
        "        # For LSTM\n",
        "        return encoded[0][:, index:index+1].contiguous(), \\\n",
        "               encoded[1][:, index:index+1].contiguous()\n",
        "\n",
        "    def decode_teacher(self, hid, input_seq):\n",
        "        # Method assumes batch of size=1\n",
        "        out, _ = self.decoder(input_seq, hid)\n",
        "        out = self.output(out.data)\n",
        "        return out\n",
        "\n",
        "    def decode_one(self, hid, input_x):\n",
        "        out, new_hid = self.decoder(input_x.unsqueeze(0), hid)\n",
        "        out = self.output(out)\n",
        "        return out.squeeze(dim=0), new_hid\n",
        "\n",
        "    def decode_chain_argmax(self, hid, begin_emb, seq_len,\n",
        "                            stop_at_token=None):\n",
        "        \"\"\"\n",
        "        Decode sequence by feeding predicted token to the net again. Act greedily\n",
        "        \"\"\"\n",
        "        res_logits = []\n",
        "        res_tokens = []\n",
        "        cur_emb = begin_emb\n",
        "\n",
        "        for _ in range(seq_len):\n",
        "            out_logits, hid = self.decode_one(hid, cur_emb)\n",
        "            out_token_v = torch.max(out_logits, dim=1)[1]\n",
        "            out_token = out_token_v.data.cpu().numpy()[0]\n",
        "\n",
        "            cur_emb = self.emb(out_token_v)\n",
        "\n",
        "            res_logits.append(out_logits)\n",
        "            res_tokens.append(out_token)\n",
        "            if stop_at_token is not None:\n",
        "                if out_token == stop_at_token:\n",
        "                    break\n",
        "        return torch.cat(res_logits), res_tokens\n",
        "\n",
        "    def decode_chain_sampling(self, hid, begin_emb, seq_len,\n",
        "                              stop_at_token=None):\n",
        "        \"\"\"\n",
        "        Decode sequence by feeding predicted token to the net again.\n",
        "        Act according to probabilities\n",
        "        \"\"\"\n",
        "        res_logits = []\n",
        "        res_actions = []\n",
        "        cur_emb = begin_emb\n",
        "\n",
        "        for _ in range(seq_len):\n",
        "            out_logits, hid = self.decode_one(hid, cur_emb)\n",
        "            out_probs_v = F.softmax(out_logits, dim=1)\n",
        "            out_probs = out_probs_v.data.cpu().numpy()[0]\n",
        "            action = int(np.random.choice(\n",
        "                out_probs.shape[0], p=out_probs))\n",
        "            action_v = torch.LongTensor([action])\n",
        "            action_v = action_v.to(begin_emb.device)\n",
        "            cur_emb = self.emb(action_v)\n",
        "\n",
        "            res_logits.append(out_logits)\n",
        "            res_actions.append(action)\n",
        "            if stop_at_token is not None:\n",
        "                if action == stop_at_token:\n",
        "                    break\n",
        "        return torch.cat(res_logits), res_actions\n",
        "\n",
        "\n",
        "def pack_batch_no_out(batch, embeddings, device=\"cuda\"):\n",
        "    assert isinstance(batch, list)\n",
        "    # Sort descending (CuDNN requirements)\n",
        "    batch.sort(key=lambda s: len(s[0]), reverse=True)\n",
        "    input_idx, output_idx = zip(*batch)\n",
        "    # create padded matrix of inputs\n",
        "    lens = list(map(len, input_idx))\n",
        "    input_mat = np.zeros((len(batch), lens[0]), dtype=np.int64)\n",
        "    for idx, x in enumerate(input_idx):\n",
        "        input_mat[idx, :len(x)] = x\n",
        "    input_v = torch.tensor(input_mat).to(device)\n",
        "    input_seq = rnn_utils.pack_padded_sequence(\n",
        "        input_v, lens, batch_first=True)\n",
        "    # lookup embeddings\n",
        "    r = embeddings(input_seq.data)\n",
        "    emb_input_seq = rnn_utils.PackedSequence(\n",
        "        r, input_seq.batch_sizes)\n",
        "    return emb_input_seq, input_idx, output_idx\n",
        "\n",
        "\n",
        "def pack_input(input_data, embeddings, device=\"cuda\"):\n",
        "    input_v = torch.LongTensor([input_data]).to(device)\n",
        "    r = embeddings(input_v)\n",
        "    return rnn_utils.pack_padded_sequence(\n",
        "        r, [len(input_data)], batch_first=True)\n",
        "\n",
        "\n",
        "def pack_batch(batch, embeddings, device=\"cuda\"):\n",
        "    emb_input_seq, input_idx, output_idx = pack_batch_no_out(\n",
        "        batch, embeddings, device)\n",
        "\n",
        "    # prepare output sequences, with end token stripped\n",
        "    output_seq_list = []\n",
        "    for out in output_idx:\n",
        "        s = pack_input(out[:-1], embeddings, device)\n",
        "        output_seq_list.append(s)\n",
        "    return emb_input_seq, output_seq_list, input_idx, output_idx\n",
        "\n",
        "\n",
        "def seq_bleu(model_out, ref_seq):\n",
        "    model_seq = torch.max(model_out.data, dim=1)[1]\n",
        "    model_seq = model_seq.cpu().numpy()\n",
        "    return calc_bleu(model_seq, ref_seq)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZdvEerptk7s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8de94c38-ffef-48a5-f13b-7472bd0ad65e"
      },
      "source": [
        "#train_crossent\n",
        "\n",
        "SAVES_DIR = \"content/drive/MyDrive/Saves\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "MAX_EPOCHES = 100\n",
        "\n",
        "log = logging.getLogger(\"train\")\n",
        "\n",
        "TEACHER_PROB = 0.5\n",
        "\n",
        "data = \"comedy\"\n",
        "name = \"crossent-comedy\"\n",
        "\n",
        "\n",
        "def run_test(test_data, net, end_token, device=\"cuda\"):\n",
        "    bleu_sum = 0.0\n",
        "    bleu_count = 0\n",
        "    for p1, p2 in test_data:\n",
        "        input_seq = pack_input(p1, net.emb, device)\n",
        "        enc = net.encode(input_seq)\n",
        "        _, tokens = net.decode_chain_argmax(\n",
        "            enc, input_seq.data[0:1], seq_len=MAX_TOKENS,\n",
        "            stop_at_token=end_token)\n",
        "        bleu_sum += calc_bleu(tokens, p2[1:])\n",
        "        bleu_count += 1\n",
        "    return bleu_sum / bleu_count\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fmt = \"%(asctime)-15s %(levelname)s %(message)s\"\n",
        "    logging.basicConfig(format=fmt, level=logging.INFO)\n",
        "    #parser = argparse.ArgumentParser()\n",
        "    #parser.add_argument(\n",
        "        #\"--data\", required=True,\n",
        "        #help=\"Category to use for training. Empty \"\n",
        "             #\"string to train on full dataset\")\n",
        "    #parser.add_argument(\n",
        "        #\"--cuda\", action='store_true', default=False,\n",
        "        #help=\"Enable cuda\")\n",
        "    #parser.add_argument(\n",
        "        #\"-n\", \"--name\", required=True, help=\"Name of the run\")\n",
        "    #args = parser.parse_args()\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    saves_path = os.path.join(SAVES_DIR, name)\n",
        "    os.makedirs(saves_path, exist_ok=True)\n",
        "\n",
        "    phrase_pairs, emb_dict = load_data(genre_filter=data)\n",
        "    log.info(\"Obtained %d phrase pairs with %d uniq words\",\n",
        "             len(phrase_pairs), len(emb_dict))\n",
        "    save_emb_dict(saves_path, emb_dict)\n",
        "    end_token = emb_dict[END_TOKEN]\n",
        "    train_data = encode_phrase_pairs(phrase_pairs, emb_dict)\n",
        "    rand = np.random.RandomState(SHUFFLE_SEED)\n",
        "    rand.shuffle(train_data)\n",
        "    log.info(\"Training data converted, got %d samples\",\n",
        "             len(train_data))\n",
        "    train_data, test_data = split_train_test(train_data)\n",
        "    log.info(\"Train set has %d phrases, test %d\",\n",
        "             len(train_data), len(test_data))\n",
        "\n",
        "    net = PhraseModel(\n",
        "        emb_size=EMBEDDING_DIM, dict_size=len(emb_dict),\n",
        "        hid_size=HIDDEN_STATE_SIZE).to(device)\n",
        "    log.info(\"Model: %s\", net)\n",
        "\n",
        "    writer = SummaryWriter(comment=\"-\" + name)\n",
        "\n",
        "    optimiser = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "    best_bleu = None\n",
        "    for epoch in range(MAX_EPOCHES):\n",
        "        losses = []\n",
        "        bleu_sum = 0.0\n",
        "        bleu_count = 0\n",
        "        for batch in iterate_batches(train_data, BATCH_SIZE):\n",
        "            optimiser.zero_grad()\n",
        "            input_seq, out_seq_list, _, out_idx = \\\n",
        "                pack_batch(batch, net.emb, device)\n",
        "            enc = net.encode(input_seq)\n",
        "\n",
        "            net_results = []\n",
        "            net_targets = []\n",
        "            for idx, out_seq in enumerate(out_seq_list):\n",
        "                ref_indices = out_idx[idx][1:]\n",
        "                enc_item = net.get_encoded_item(enc, idx)\n",
        "                if random.random() < TEACHER_PROB:\n",
        "                    r = net.decode_teacher(enc_item, out_seq)\n",
        "                    bleu_sum += seq_bleu(r, ref_indices)\n",
        "                else:\n",
        "                    r, seq = net.decode_chain_argmax(\n",
        "                        enc_item, out_seq.data[0:1],\n",
        "                        len(ref_indices))\n",
        "                    bleu_sum += calc_bleu(seq, ref_indices)\n",
        "                net_results.append(r)\n",
        "                net_targets.extend(ref_indices)\n",
        "                bleu_count += 1\n",
        "            results_v = torch.cat(net_results)\n",
        "            targets_v = torch.LongTensor(net_targets).to(device)\n",
        "            loss_v = F.cross_entropy(results_v, targets_v)\n",
        "            loss_v.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            losses.append(loss_v.item())\n",
        "        bleu = bleu_sum / bleu_count\n",
        "        bleu_test = run_test(test_data, net, end_token, device)\n",
        "        log.info(\"Epoch %d: mean loss %.3f, mean BLEU %.3f, \"\n",
        "                 \"test BLEU %.3f\", epoch, np.mean(losses),\n",
        "                 bleu, bleu_test)\n",
        "        writer.add_scalar(\"loss\", np.mean(losses), epoch)\n",
        "        writer.add_scalar(\"bleu\", bleu, epoch)\n",
        "        writer.add_scalar(\"bleu_test\", bleu_test, epoch)\n",
        "        if best_bleu is None or best_bleu < bleu_test:\n",
        "            if best_bleu is not None:\n",
        "                out_name = os.path.join(\n",
        "                    saves_path, \"pre_bleu_%.3f_%02d.dat\" % (\n",
        "                        bleu_test, epoch))\n",
        "                torch.save(net.state_dict(), out_name)\n",
        "                log.info(\"Best BLEU updated %.3f\", bleu_test)\n",
        "            best_bleu = bleu_test\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            out_name = os.path.join(\n",
        "                saves_path, \"epoch_%03d_%.3f_%.3f.dat\" % (\n",
        "                    epoch, bleu, bleu_test))\n",
        "            torch.save(net.state_dict(), out_name)\n",
        "\n",
        "    writer.close()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-12-29 16:42:47,998 INFO Loaded 159 movies with genre comedy\n",
            "2021-12-29 16:42:48,001 INFO Read and tokenise phrases...\n",
            "2021-12-29 16:42:54,839 INFO Loaded 93039 phrases\n",
            "2021-12-29 16:42:55,155 INFO Loaded 24716 dialogues with 93039 phrases, generating training pairs\n",
            "2021-12-29 16:42:55,203 INFO Counting freq of words...\n",
            "2021-12-29 16:42:55,625 INFO Data has 31774 uniq words, 4913 of them occur more than 10\n",
            "2021-12-29 16:42:55,872 INFO Obtained 47644 phrase pairs with 4905 uniq words\n",
            "2021-12-29 16:42:56,554 INFO Training data converted, got 26491 samples\n",
            "2021-12-29 16:42:56,556 INFO Train set has 25166 phrases, test 1325\n",
            "2021-12-29 16:42:56,610 INFO Model: PhraseModel(\n",
            "  (emb): Embedding(4905, 50)\n",
            "  (encoder): LSTM(50, 512, batch_first=True)\n",
            "  (decoder): LSTM(50, 512, batch_first=True)\n",
            "  (output): Linear(in_features=512, out_features=4905, bias=True)\n",
            ")\n",
            "2021-12-29 16:45:25,680 INFO Epoch 0: mean loss 5.009, mean BLEU 0.155, test BLEU 0.120\n",
            "2021-12-29 16:47:55,357 INFO Epoch 1: mean loss 4.686, mean BLEU 0.172, test BLEU 0.102\n",
            "2021-12-29 16:50:23,766 INFO Epoch 2: mean loss 4.545, mean BLEU 0.176, test BLEU 0.084\n",
            "2021-12-29 16:52:51,590 INFO Epoch 3: mean loss 4.438, mean BLEU 0.181, test BLEU 0.089\n",
            "2021-12-29 16:55:22,182 INFO Epoch 4: mean loss 4.368, mean BLEU 0.184, test BLEU 0.104\n",
            "2021-12-29 16:57:50,242 INFO Epoch 5: mean loss 4.252, mean BLEU 0.192, test BLEU 0.090\n",
            "2021-12-29 17:00:22,791 INFO Epoch 6: mean loss 4.168, mean BLEU 0.194, test BLEU 0.093\n",
            "2021-12-29 17:02:52,939 INFO Epoch 7: mean loss 4.089, mean BLEU 0.202, test BLEU 0.097\n",
            "2021-12-29 17:05:16,606 INFO Epoch 8: mean loss 3.978, mean BLEU 0.210, test BLEU 0.110\n",
            "2021-12-29 17:07:41,172 INFO Epoch 9: mean loss 3.913, mean BLEU 0.215, test BLEU 0.111\n",
            "2021-12-29 17:10:09,461 INFO Epoch 10: mean loss 3.826, mean BLEU 0.227, test BLEU 0.100\n",
            "2021-12-29 17:12:33,322 INFO Epoch 11: mean loss 3.748, mean BLEU 0.234, test BLEU 0.101\n",
            "2021-12-29 17:15:43,122 INFO Epoch 12: mean loss 3.670, mean BLEU 0.243, test BLEU 0.101\n",
            "2021-12-29 17:18:18,349 INFO Epoch 13: mean loss 3.595, mean BLEU 0.255, test BLEU 0.109\n",
            "2021-12-29 17:20:57,694 INFO Epoch 14: mean loss 3.529, mean BLEU 0.267, test BLEU 0.110\n",
            "2021-12-29 17:23:34,090 INFO Epoch 15: mean loss 3.452, mean BLEU 0.280, test BLEU 0.115\n",
            "2021-12-29 17:25:57,617 INFO Epoch 16: mean loss 3.373, mean BLEU 0.292, test BLEU 0.108\n",
            "2021-12-29 17:28:19,904 INFO Epoch 17: mean loss 3.321, mean BLEU 0.306, test BLEU 0.108\n",
            "2021-12-29 17:30:44,177 INFO Epoch 18: mean loss 3.279, mean BLEU 0.317, test BLEU 0.113\n",
            "2021-12-29 17:33:08,077 INFO Epoch 19: mean loss 3.214, mean BLEU 0.328, test BLEU 0.109\n",
            "2021-12-29 17:35:32,776 INFO Epoch 20: mean loss 3.166, mean BLEU 0.338, test BLEU 0.113\n",
            "2021-12-29 17:37:57,352 INFO Epoch 21: mean loss 3.102, mean BLEU 0.353, test BLEU 0.112\n",
            "2021-12-29 17:40:21,415 INFO Epoch 22: mean loss 3.047, mean BLEU 0.366, test BLEU 0.112\n",
            "2021-12-29 17:42:46,974 INFO Epoch 23: mean loss 2.993, mean BLEU 0.379, test BLEU 0.113\n",
            "2021-12-29 17:45:12,498 INFO Epoch 24: mean loss 2.933, mean BLEU 0.391, test BLEU 0.107\n",
            "2021-12-29 17:47:35,668 INFO Epoch 25: mean loss 2.889, mean BLEU 0.403, test BLEU 0.113\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7590258e24f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m                     r, seq = net.decode_chain_argmax(\n\u001b[1;32m     92\u001b[0m                         \u001b[0menc_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                         len(ref_indices))\n\u001b[0m\u001b[1;32m     94\u001b[0m                     \u001b[0mbleu_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalc_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mnet_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e5fe4cdd1a7d>\u001b[0m in \u001b[0;36mdecode_chain_argmax\u001b[0;34m(self, hid, begin_emb, seq_len, stop_at_token)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mout_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mout_token_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_token_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e5fe4cdd1a7d>\u001b[0m in \u001b[0;36mdecode_one\u001b[0;34m(self, hid, input_x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_hid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_hid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 692\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PTAN\n",
        "\n",
        "class TBMeanTracker:\n",
        "    \"\"\"\n",
        "    TensorBoard value tracker: allows to batch fixed amount of historical values and write their mean into TB\n",
        "    Designed and tested with pytorch-tensorboard in mind\n",
        "    \"\"\"\n",
        "    def __init__(self, writer, batch_size):\n",
        "        \"\"\"\n",
        "        :param writer: writer with close() and add_scalar() methods\n",
        "        :param batch_size: integer size of batch to track\n",
        "        \"\"\"\n",
        "        assert isinstance(batch_size, int)\n",
        "        assert writer is not None\n",
        "        self.writer = writer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __enter__(self):\n",
        "        self._batches = collections.defaultdict(list)\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.writer.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def _as_float(value):\n",
        "        assert isinstance(value, (float, int, np.ndarray, np.generic, torch.autograd.Variable)) or torch.is_tensor(value)\n",
        "        tensor_val = None\n",
        "        if isinstance(value, torch.autograd.Variable):\n",
        "            tensor_val = value.data\n",
        "        elif torch.is_tensor(value):\n",
        "            tensor_val = value\n",
        "\n",
        "        if tensor_val is not None:\n",
        "            return tensor_val.float().mean().item()\n",
        "        elif isinstance(value, np.ndarray):\n",
        "            return float(np.mean(value))\n",
        "        else:\n",
        "            return float(value)\n",
        "\n",
        "    def track(self, param_name, value, iter_index):\n",
        "        assert isinstance(param_name, str)\n",
        "        assert isinstance(iter_index, int)\n",
        "\n",
        "        data = self._batches[param_name]\n",
        "        data.append(self._as_float(value))\n",
        "\n",
        "        if len(data) >= self.batch_size:\n",
        "            self.writer.add_scalar(param_name, np.mean(data), iter_index)\n",
        "            data.clear()"
      ],
      "metadata": {
        "id": "2DqA7mu4NHMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfp0mvmLt1y7"
      },
      "source": [
        "#train_scst\n",
        "\n",
        "import ptan\n",
        "\n",
        "SAVES_DIR = \"saves\"\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 5e-4\n",
        "MAX_EPOCHES = 10000\n",
        "\n",
        "log = logging.getLogger(\"train\")\n",
        "\n",
        "\n",
        "def run_test(test_data, net, end_token, device=\"cpu\"):\n",
        "    bleu_sum = 0.0\n",
        "    bleu_count = 0\n",
        "    for p1, p2 in test_data:\n",
        "        input_seq = pack_input(p1, net.emb, device)\n",
        "        enc = net.encode(input_seq)\n",
        "        _, tokens = net.decode_chain_argmax(\n",
        "            enc, input_seq.data[0:1], seq_len=data.MAX_TOKENS,\n",
        "            stop_at_token=end_token)\n",
        "        ref_indices = [\n",
        "            indices[1:]\n",
        "            for indices in p2\n",
        "        ]\n",
        "        bleu_sum += calc_bleu_many(tokens, ref_indices)\n",
        "        bleu_count += 1\n",
        "    return bleu_sum / bleu_count\n",
        "\n",
        "name2 = \"rl-comedy\"\n",
        "disable_skip = False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #parser = argparse.ArgumentParser()\n",
        "    fmt = \"%(asctime)-15s %(levelname)s %(message)s\"\n",
        "    #logging.basicConfig(format=fmt, level=logging.INFO)\n",
        "    #parser.add_argument(\n",
        "        #\"--data\", required=True,\n",
        "        #help=\"Category to use for training. Empty \"\n",
        "             #\"string to train on full dataset\")\n",
        "    #parser.add_argument(\n",
        "        #\"--cuda\", action='store_true', default=False,\n",
        "        #help=\"Enable cuda\")\n",
        "    #parser.add_argument(\n",
        "        #\"-n\", \"--name\", required=True,\n",
        "        #help=\"Name of the run\")\n",
        "    #parser.add_argument(\n",
        "        #\"-l\", \"--load\", required=True,\n",
        "        #help=\"Load model and continue in RL mode\")\n",
        "    #parser.add_argument(\n",
        "        #\"--samples\", type=int, default=4,\n",
        "        #help=\"Count of samples in prob mode\")\n",
        "    #parser.add_argument(\n",
        "        #\"--disable-skip\", default=False, action='store_true',\n",
        "        #help=\"Disable skipping of samples with high argmax BLEU\")\n",
        "    #args = parser.parse_args()\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    saves_path = os.path.join(SAVES_DIR, name2)\n",
        "    os.makedirs(saves_path, exist_ok=True)\n",
        "\n",
        "    phrase_pairs, emb_dict = \\\n",
        "        load_data(genre_filter=data)\n",
        "    log.info(\"Obtained %d phrase pairs with %d uniq words\",\n",
        "             len(phrase_pairs), len(emb_dict))\n",
        "    data.save_emb_dict(saves_path, emb_dict)\n",
        "    end_token = emb_dict[END_TOKEN]\n",
        "    train_data = data.encode_phrase_pairs(phrase_pairs, emb_dict)\n",
        "    rand = np.random.RandomState(SHUFFLE_SEED)\n",
        "    rand.shuffle(train_data)\n",
        "    train_data, test_data = split_train_test(train_data)\n",
        "    log.info(\"Training data converted, got %d samples\",\n",
        "             len(train_data))\n",
        "    train_data = group_train_data(train_data)\n",
        "    test_data = group_train_data(test_data)\n",
        "    log.info(\"Train set has %d phrases, test %d\",\n",
        "             len(train_data), len(test_data))\n",
        "\n",
        "    rev_emb_dict = {idx: word for word, idx in emb_dict.items()}\n",
        "\n",
        "    net = PhraseModel(\n",
        "        emb_size=EMBEDDING_DIM, dict_size=len(emb_dict),\n",
        "        hid_size=HIDDEN_STATE_SIZE).to(device)\n",
        "    log.info(\"Model: %s\", net)\n",
        "\n",
        "    writer = SummaryWriter(comment=\"-\" + name2)\n",
        "    net.load_state_dict(torch.load(name))\n",
        "    log.info(\"Model loaded from %s, continue \"\n",
        "             \"training in RL mode...\", name)\n",
        "\n",
        "    # BEGIN token\n",
        "    beg_token = torch.LongTensor([emb_dict[BEGIN_TOKEN]])\n",
        "    beg_token = beg_token.to(device)\n",
        "\n",
        "    with TBMeanTracker(\n",
        "            writer, 100) as tb_tracker:\n",
        "        optimiser = optim.Adam(\n",
        "            net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n",
        "        batch_idx = 0\n",
        "        best_bleu = None\n",
        "        for epoch in range(MAX_EPOCHES):\n",
        "            random.shuffle(train_data)\n",
        "            dial_shown = False\n",
        "\n",
        "            total_samples = 0\n",
        "            skipped_samples = 0\n",
        "            bleus_argmax = []\n",
        "            bleus_sample = []\n",
        "\n",
        "            for batch in data.iterate_batches(\n",
        "                    train_data, BATCH_SIZE):\n",
        "                batch_idx += 1\n",
        "                optimiser.zero_grad()\n",
        "                input_seq, input_batch, output_batch = \\\n",
        "                    pack_batch_no_out(batch, net.emb, device)\n",
        "                enc = net.encode(input_seq)\n",
        "\n",
        "                net_policies = []\n",
        "                net_actions = []\n",
        "                net_advantages = []\n",
        "                beg_embedding = net.emb(beg_token)\n",
        "\n",
        "                for idx, inp_idx in enumerate(input_batch):\n",
        "                    total_samples += 1\n",
        "                    ref_indices = [\n",
        "                        indices[1:]\n",
        "                        for indices in output_batch[idx]\n",
        "                    ]\n",
        "                    item_enc = net.get_encoded_item(enc, idx)\n",
        "                    r_argmax, actions = net.decode_chain_argmax(\n",
        "                        item_enc, beg_embedding, MAX_TOKENS,\n",
        "                        stop_at_token=end_token)\n",
        "                    argmax_bleu = calc_bleu_many(\n",
        "                        actions, ref_indices)\n",
        "                    bleus_argmax.append(argmax_bleu)\n",
        "\n",
        "                    if not disable_skip:\n",
        "                        if argmax_bleu > 0.99:\n",
        "                            skipped_samples += 1\n",
        "                            continue\n",
        "\n",
        "                    if not dial_shown:\n",
        "                        w = decode_words(\n",
        "                            inp_idx, rev_emb_dict)\n",
        "                        log.info(\"Input: %s\", untokenize(w))\n",
        "                        ref_words = [\n",
        "                            utils.untokenize(\n",
        "                                decode_words(\n",
        "                                    ref, rev_emb_dict))\n",
        "                            for ref in ref_indices\n",
        "                        ]\n",
        "                        ref = \" ~~|~~ \".join(ref_words)\n",
        "                        log.info(\"Refer: %s\", ref)\n",
        "                        w = decode_words(\n",
        "                            actions, rev_emb_dict)\n",
        "                        log.info(\"Argmax: %s, bleu=%.4f\",\n",
        "                                 untokenize(w), argmax_bleu)\n",
        "\n",
        "                    for _ in range(args.samples):\n",
        "                        r_sample, actions = \\\n",
        "                            net.decode_chain_sampling(\n",
        "                                item_enc, beg_embedding,\n",
        "                                data.MAX_TOKENS,\n",
        "                                stop_at_token=end_token)\n",
        "                        sample_bleu = utils.calc_bleu_many(\n",
        "                            actions, ref_indices)\n",
        "\n",
        "                        if not dial_shown:\n",
        "                            w = data.decode_words(\n",
        "                                actions, rev_emb_dict)\n",
        "                            log.info(\"Sample: %s, bleu=%.4f\",\n",
        "                                     utils.untokenize(w),\n",
        "                                     sample_bleu)\n",
        "\n",
        "                        net_policies.append(r_sample)\n",
        "                        net_actions.extend(actions)\n",
        "                        adv = sample_bleu - argmax_bleu\n",
        "                        net_advantages.extend(\n",
        "                            [adv]*len(actions))\n",
        "                        bleus_sample.append(sample_bleu)\n",
        "                    dial_shown = True\n",
        "\n",
        "                if not net_policies:\n",
        "                    continue\n",
        "\n",
        "                policies_v = torch.cat(net_policies)\n",
        "                actions_t = torch.LongTensor(\n",
        "                    net_actions).to(device)\n",
        "                adv_v = torch.FloatTensor(\n",
        "                    net_advantages).to(device)\n",
        "                log_prob_v = F.log_softmax(policies_v, dim=1)\n",
        "                lp_a = log_prob_v[range(len(net_actions)),\n",
        "                                  actions_t]\n",
        "                log_prob_actions_v = adv_v * lp_a\n",
        "                loss_policy_v = -log_prob_actions_v.mean()\n",
        "\n",
        "                loss_v = loss_policy_v\n",
        "                loss_v.backward()\n",
        "                optimiser.step()\n",
        "\n",
        "                tb_tracker.track(\"advantage\", adv_v, batch_idx)\n",
        "                tb_tracker.track(\"loss_policy\", loss_policy_v,\n",
        "                                 batch_idx)\n",
        "                tb_tracker.track(\"loss_total\", loss_v, batch_idx)\n",
        "\n",
        "            bleu_test = run_test(test_data, net,\n",
        "                                 end_token, device)\n",
        "            bleu = np.mean(bleus_argmax)\n",
        "            writer.add_scalar(\"bleu_test\", bleu_test, batch_idx)\n",
        "            writer.add_scalar(\"bleu_argmax\", bleu, batch_idx)\n",
        "            writer.add_scalar(\"bleu_sample\",\n",
        "                              np.mean(bleus_sample), batch_idx)\n",
        "            writer.add_scalar(\"skipped_samples\",\n",
        "                              skipped_samples / total_samples,\n",
        "                              batch_idx)\n",
        "            writer.add_scalar(\"epoch\", batch_idx, epoch)\n",
        "            log.info(\"Epoch %d, test BLEU: %.3f\",\n",
        "                     epoch, bleu_test)\n",
        "            if best_bleu is None or best_bleu < bleu_test:\n",
        "                best_bleu = bleu_test\n",
        "                log.info(\"Best bleu updated: %.4f\", bleu_test)\n",
        "                torch.save(net.state_dict(), os.path.join(\n",
        "                    saves_path, \"bleu_%.3f_%02d.dat\" % (\n",
        "                        bleu_test, epoch)))\n",
        "            if epoch % 10 == 0:\n",
        "                torch.save(net.state_dict(), os.path.join(\n",
        "                    saves_path, \"epoch_%03d_%.3f_%.3f.dat\" % (\n",
        "                        epoch, bleu, bleu_test)))\n",
        "\n",
        "    writer.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMinucLBuRb_"
      },
      "source": [
        "from unittest import TestCase\n",
        "\n",
        "import libbots.data\n",
        "from libbots import data, subtitles\n",
        "\n",
        "\n",
        "class TestData(TestCase):\n",
        "    emb_dict = {\n",
        "        data.BEGIN_TOKEN: 0,\n",
        "        data.END_TOKEN: 1,\n",
        "        data.UNKNOWN_TOKEN: 2,\n",
        "        'a': 3,\n",
        "        'b': 4\n",
        "    }\n",
        "\n",
        "    def test_encode_words(self):\n",
        "        res = data.encode_words(['a', 'b', 'c'], self.emb_dict)\n",
        "        self.assertEqual(res, [0, 3, 4, 2, 1])\n",
        "\n",
        "    # def test_dialogues_to_train(self):\n",
        "    #     dialogues = [\n",
        "    #         [\n",
        "    #             libbots.data.Phrase(words=['a', 'b'], time_start=0, time_stop=1),\n",
        "    #             libbots.data.Phrase(words=['b', 'a'], time_start=2, time_stop=3),\n",
        "    #             libbots.data.Phrase(words=['b', 'a'], time_start=2, time_stop=3),\n",
        "    #         ],\n",
        "    #         [\n",
        "    #             libbots.data.Phrase(words=['a', 'b'], time_start=0, time_stop=1),\n",
        "    #         ]\n",
        "    #     ]\n",
        "    #\n",
        "    #     res = data.dialogues_to_train(dialogues, self.emb_dict)\n",
        "    #     self.assertEqual(res, [\n",
        "    #         ([0, 3, 4, 1], [0, 4, 3, 1]),\n",
        "    #         ([0, 4, 3, 1], [0, 4, 3, 1]),\n",
        "    #     ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkzJr9nEuUMm"
      },
      "source": [
        "#test_subtitles\n",
        "\n",
        "import datetime\n",
        "from unittest import TestCase\n",
        "\n",
        "import libbots.data\n",
        "from libbots import subtitles\n",
        "\n",
        "\n",
        "class TestPhrases(TestCase):\n",
        "    def test_split_phrase(self):\n",
        "        phrase = libbots.data.Phrase(words=[\"a\", \"b\", \"c\"], time_start=datetime.timedelta(seconds=0),\n",
        "                                     time_stop=datetime.timedelta(seconds=10))\n",
        "        res = subtitles.split_phrase(phrase)\n",
        "        self.assertIsInstance(res, list)\n",
        "        self.assertEqual(len(res), 1)\n",
        "        self.assertEqual(res[0], phrase)\n",
        "\n",
        "        phrase = libbots.data.Phrase(words=[\"a\", \"b\", \"-\", \"c\"], time_start=datetime.timedelta(seconds=0),\n",
        "                                     time_stop=datetime.timedelta(seconds=10))\n",
        "        res = subtitles.split_phrase(phrase)\n",
        "        self.assertEqual(len(res), 2)\n",
        "        self.assertEqual(res[0].words, [\"a\", \"b\"])\n",
        "        self.assertEqual(res[1].words, [\"c\"])\n",
        "        self.assertAlmostEqual(res[0].time_start.total_seconds(), 0)\n",
        "        self.assertAlmostEqual(res[0].time_stop.total_seconds(), 5)\n",
        "        self.assertAlmostEqual(res[1].time_start.total_seconds(), 5)\n",
        "        self.assertAlmostEqual(res[1].time_stop.total_seconds(), 10)\n",
        "\n",
        "        phrase = libbots.data.Phrase(words=['-', 'Wait', 'a', 'sec', '.', '-'], time_start=datetime.timedelta(0, 588, 204000),\n",
        "                                     time_stop=datetime.timedelta(0, 590, 729000))\n",
        "        res = subtitles.split_phrase(phrase)\n",
        "        self.assertEqual(res[0].words, [\"Wait\", \"a\", \"sec\", \".\"])\n",
        "\n",
        "\n",
        "class TestUtils(TestCase):\n",
        "    def test_parse_time(self):\n",
        "        self.assertEqual(subtitles.parse_time(\"00:00:33,074\"),\n",
        "                         datetime.timedelta(seconds=33, milliseconds=74))\n",
        "\n",
        "    def test_remove_braced_words(self):\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', 'b', 'c']),\n",
        "                         ['a', 'b', 'c'])\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', '[', 'b', ']', 'c']),\n",
        "                         ['a', 'c'])\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', '[', 'b', 'c']),\n",
        "                         ['a'])\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', ']', 'b', 'c']),\n",
        "                         ['a', 'b', 'c'])\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', '(', 'b', ']', 'c']),\n",
        "                         ['a', 'c'])\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', '(', 'b', 'c']),\n",
        "                         ['a'])\n",
        "        self.assertEqual(subtitles.remove_braced_words(['a', ')', 'b', 'c']),\n",
        "                         ['a', 'b', 'c'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW0m5hDUuwkl"
      },
      "source": [
        "#test_data\n",
        "\n",
        "import libbots.data\n",
        "from libbots import data, subtitles\n",
        "\n",
        "\n",
        "class TestData(TestCase):\n",
        "    emb_dict = {\n",
        "        data.BEGIN_TOKEN: 0,\n",
        "        data.END_TOKEN: 1,\n",
        "        data.UNKNOWN_TOKEN: 2,\n",
        "        'a': 3,\n",
        "        'b': 4\n",
        "    }\n",
        "\n",
        "    def test_encode_words(self):\n",
        "        res = data.encode_words(['a', 'b', 'c'], self.emb_dict)\n",
        "        self.assertEqual(res, [0, 3, 4, 2, 1])\n",
        "\n",
        "    # def test_dialogues_to_train(self):\n",
        "    #     dialogues = [\n",
        "    #         [\n",
        "    #             libbots.data.Phrase(words=['a', 'b'], time_start=0, time_stop=1),\n",
        "    #             libbots.data.Phrase(words=['b', 'a'], time_start=2, time_stop=3),\n",
        "    #             libbots.data.Phrase(words=['b', 'a'], time_start=2, time_stop=3),\n",
        "    #         ],\n",
        "    #         [\n",
        "    #             libbots.data.Phrase(words=['a', 'b'], time_start=0, time_stop=1),\n",
        "    #         ]\n",
        "    #     ]\n",
        "    #\n",
        "    #     res = data.dialogues_to_train(dialogues, self.emb_dict)\n",
        "    #     self.assertEqual(res, [\n",
        "    #         ([0, 3, 4, 1], [0, 4, 3, 1]),\n",
        "    #         ([0, 4, 3, 1], [0, 4, 3, 1]),\n",
        "    #     ])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data_test\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "from libbots import data, model, utils\n",
        "\n",
        "import torch\n",
        "\n",
        "log = logging.getLogger(\"data_test\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(format=\"%(asctime)-15s %(levelname)s %(message)s\", level=logging.INFO)\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", required=True,\n",
        "                        help=\"Category to filter, empty string will use the full dataset\")\n",
        "    parser.add_argument(\"-m\", \"--model\", required=True, help=\"Model name to load\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    phrase_pairs, emb_dict = data.load_data(args.data)\n",
        "    log.info(\"Obtained %d phrase pairs with %d uniq words\", len(phrase_pairs), len(emb_dict))\n",
        "    train_data = data.encode_phrase_pairs(phrase_pairs, emb_dict)\n",
        "    train_data = data.group_train_data(train_data)\n",
        "    rev_emb_dict = {idx: word for word, idx in emb_dict.items()}\n",
        "\n",
        "    net = model.PhraseModel(emb_size=model.EMBEDDING_DIM, dict_size=len(emb_dict), hid_size=model.HIDDEN_STATE_SIZE)\n",
        "    net.load_state_dict(torch.load(args.model))\n",
        "\n",
        "    end_token = emb_dict[data.END_TOKEN]\n",
        "\n",
        "    seq_count = 0\n",
        "    sum_bleu = 0.0\n",
        "\n",
        "    for seq_1, targets in train_data:\n",
        "        input_seq = model.pack_input(seq_1, net.emb)\n",
        "        enc = net.encode(input_seq)\n",
        "        _, tokens = net.decode_chain_argmax(enc, input_seq.data[0:1],\n",
        "                                            seq_len=data.MAX_TOKENS, stop_at_token=end_token)\n",
        "        references = [seq[1:] for seq in targets]\n",
        "        bleu = utils.calc_bleu_many(tokens, references)\n",
        "        sum_bleu += bleu\n",
        "        seq_count += 1\n",
        "\n",
        "    log.info(\"Processed %d phrases, mean BLEU = %.4f\", seq_count, sum_bleu / seq_count)\n"
      ],
      "metadata": {
        "id": "JV0-9vWn74eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "from libbots import data, model, utils\n",
        "\n",
        "import torch\n",
        "\n",
        "log = logging.getLogger(\"data_test\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(format=\"%(asctime)-15s %(levelname)s %(message)s\", level=logging.INFO)\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", required=True,\n",
        "                        help=\"Category to filter, empty string will use the full dataset\")\n",
        "    parser.add_argument(\"-m\", \"--model\", required=True, help=\"Model name to load\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    phrase_pairs, emb_dict = data.load_data(args.data)\n",
        "    log.info(\"Obtained %d phrase pairs with %d uniq words\", len(phrase_pairs), len(emb_dict))\n",
        "    train_data = data.encode_phrase_pairs(phrase_pairs, emb_dict)\n",
        "    train_data = data.group_train_data(train_data)\n",
        "    rev_emb_dict = {idx: word for word, idx in emb_dict.items()}\n",
        "\n",
        "    net = model.PhraseModel(emb_size=model.EMBEDDING_DIM, dict_size=len(emb_dict), hid_size=model.HIDDEN_STATE_SIZE)\n",
        "    net.load_state_dict(torch.load(args.model))\n",
        "\n",
        "    end_token = emb_dict[data.END_TOKEN]\n",
        "\n",
        "    seq_count = 0\n",
        "    sum_bleu = 0.0\n",
        "\n",
        "    for seq_1, targets in train_data:\n",
        "        input_seq = model.pack_input(seq_1, net.emb)\n",
        "        enc = net.encode(input_seq)\n",
        "        _, tokens = net.decode_chain_argmax(enc, input_seq.data[0:1],\n",
        "                                            seq_len=data.MAX_TOKENS, stop_at_token=end_token)\n",
        "        references = [seq[1:] for seq in targets]\n",
        "        bleu = utils.calc_bleu_many(tokens, references)\n",
        "        sum_bleu += bleu\n",
        "        seq_count += 1\n",
        "\n",
        "    log.info(\"Processed %d phrases, mean BLEU = %.4f\", seq_count, sum_bleu / seq_count)\n"
      ],
      "metadata": {
        "id": "BbyPN9la8NrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use_model\n",
        "\n",
        "log = logging.getLogger(\"use\")\n",
        "string = \"Hello Clyde!\"\n",
        "sample = False\n",
        "self = False\n",
        "\n",
        "\n",
        "def words_to_words(words, emb_dict, rev_emb_dict, net, use_sampling=False):\n",
        "    tokens = encode_words(words, emb_dict)\n",
        "    input_seq = pack_input(tokens, net.emb)\n",
        "    enc = net.encode(input_seq)\n",
        "    end_token = emb_dict[END_TOKEN]\n",
        "    if use_sampling:\n",
        "        _, out_tokens = net.decode_chain_sampling(enc, input_seq.data[0:1], seq_len=MAX_TOKENS,\n",
        "                                                  stop_at_token=end_token)\n",
        "    else:\n",
        "        _, out_tokens = net.decode_chain_argmax(enc, input_seq.data[0:1], seq_len=MAX_TOKENS,\n",
        "                                                stop_at_token=end_token)\n",
        "    if out_tokens[-1] == end_token:\n",
        "        out_tokens = out_tokens[:-1]\n",
        "    out_words = decode_words(out_tokens, rev_emb_dict)\n",
        "    return out_words\n",
        "\n",
        "\n",
        "def process_string(s, emb_dict, rev_emb_dict, net, use_sampling=False):\n",
        "    out_words = words_to_words(words, emb_dict, rev_emb_dict, net, use_sampling=use_sampling)\n",
        "    print(\" \".join(out_words))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(format=\"%(asctime)-15s %(levelname)s %(message)s\", level=logging.INFO)\n",
        "    #parser = argparse.ArgumentParser()\n",
        "    #parser.add_argument(\"-m\", \"--model\", required=True, help=\"Model name to load\")\n",
        "    #parser.add_argument(\"-s\", \"--string\", help=\"String to process, otherwise will loop\")\n",
        "    #parser.add_argument(\"--sample\", default=False, action=\"store_true\", help=\"Enable sampling generation instead of argmax\")\n",
        "    #parser.add_argument(\"--self\", type=int, default=1, help=\"Enable self-loop mode with given amount of phrases.\")\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    emb_dict = load_emb_dict(os.path.dirname(DATA_DIR))\n",
        "    net = PhraseModel(emb_size=model.EMBEDDING_DIM, dict_size=len(emb_dict), hid_size=HIDDEN_STATE_SIZE)\n",
        "    net.load_state_dict(torch.load(args.model))\n",
        "\n",
        "    rev_emb_dict = {idx: word for word, idx in emb_dict.items()}\n",
        "\n",
        "    while True:\n",
        "        if args.string:\n",
        "            input_string = args.string\n",
        "        else:\n",
        "            input_string = input(\">>> \")\n",
        "        if not input_string:\n",
        "            break\n",
        "\n",
        "        words = utils.tokenize(input_string)\n",
        "        for _ in range(args.self):\n",
        "            words = words_to_words(words, emb_dict, rev_emb_dict, net, use_sampling=args.sample)\n",
        "            print(utils.untokenize(words))\n",
        "\n",
        "        if args.string:\n",
        "            break\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "hH7xUQ9W8Tyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cor_reader\n",
        "\n",
        "show_genres = False\n",
        "show_dials = False\n",
        "show_train = False\n",
        "show_dict_freq = False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #parser = argparse.ArgumentParser()\n",
        "    #parser.add_argument(\"-g\", \"--genre\", default='', help=\"Genre to show dialogs from\")\n",
        "    #parser.add_argument(\"--show-genres\", action='store_true', default=False, help=\"Display genres stats\")\n",
        "    #parser.add_argument(\"--show-dials\", action='store_true', default=False, help=\"Display dialogs\")\n",
        "    #parser.add_argument(\"--show-train\", action='store_true', default=False, help=\"Display training pairs\")\n",
        "    #parser.add_argument(\"--show-dict-freq\", action='store_true', default=False, help=\"Display dictionary frequency\")\n",
        "    #args = parser.parse_args(args=[])\n",
        "\n",
        "    if show_genres:\n",
        "        genre_counts = collections.Counter()\n",
        "        genres = read_genres(DATA_DIR)\n",
        "        for movie, g_list in genres.items():\n",
        "            for g in g_list:\n",
        "                genre_counts[g] += 1\n",
        "        print(\"Genres:\")\n",
        "        for g, count in genre_counts.most_common():\n",
        "            print(\"%s: %d\" % (g, count))\n",
        "\n",
        "    if show_dials:\n",
        "        dials = load_dialogues(genre_filter=genre)\n",
        "        for d_idx, dial in enumerate(dials):\n",
        "            print(\"Dialog %d with %d phrases:\" % (d_idx, len(dial)))\n",
        "            for p in dial:\n",
        "                print(\" \".join(p))\n",
        "            print()\n",
        "\n",
        "    if show_train or show_dict_freq:\n",
        "        phrase_pairs, emb_dict = data.load_data(genre_filter=genre)\n",
        "\n",
        "    if show_train:\n",
        "        rev_emb_dict = {idx: word for word, idx in emb_dict.items()}\n",
        "        train_data = data.encode_phrase_pairs(phrase_pairs, emb_dict)\n",
        "        train_data = data.group_train_data(train_data)\n",
        "        unk_token = emb_dict[data.UNKNOWN_TOKEN]\n",
        "\n",
        "        print(\"Training pairs (%d total)\" % len(train_data))\n",
        "        train_data.sort(key=lambda p: len(p[1]), reverse=True)\n",
        "        for idx, (p1, p2_group) in enumerate(train_data):\n",
        "            w1 = decode_words(p1, rev_emb_dict)\n",
        "            w2_group = [decode_words(p2, rev_emb_dict) for p2 in p2_group]\n",
        "            print(\"%d:\" % idx, \" \".join(w1))\n",
        "            for w2 in w2_group:\n",
        "                print(\"%s:\" % (\" \" * len(str(idx))), \" \".join(w2))\n",
        "\n",
        "    if show_dict_freq:\n",
        "        words_stat = collections.Counter()\n",
        "        for p1, p2 in phrase_pairs:\n",
        "            words_stat.update(p1)\n",
        "        print(\"Frequency stats for %d tokens in the dict\" % len(emb_dict))\n",
        "        for token, count in words_stat.most_common():\n",
        "            print(\"%s: %d\" % (token, count))\n",
        "    pass"
      ],
      "metadata": {
        "id": "WKy4FRpJ8v6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data\n",
        "urlcmd = 'https://raw.githubusercontent.com/ncr5012/EmailRL/main/cornell%20movie-dialogs%20corpus/movie_characters_metadata.txt'\n",
        "urltmd = 'https://raw.githubusercontent.com/ncr5012/EmailRL/main/cornell%20movie-dialogs%20corpus/movie_titles_metadata.txt'\n",
        "urlrs = 'https://raw.githubusercontent.com/ncr5012/EmailRL/main/cornell%20movie-dialogs%20corpus/raw_script_urls.txt'\n",
        "movie_characters_metadata = pd.read_csv(urlcmd,sep='\\+\\+\\+\\$\\+\\+\\+',engine='python', encoding='latin-1')\n",
        "movie_titles_metadata = pd.read_csv(urltmd,sep='\\+\\+\\+\\$\\+\\+\\+',engine='python', encoding='latin-1')\n",
        "movie_raw_script_urls = pd.read_csv(urlrs,sep='\\+\\+\\+\\$\\+\\+\\+',engine='python', encoding='latin-1')\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "metadata": {
        "id": "NaQURVyG9hBr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}